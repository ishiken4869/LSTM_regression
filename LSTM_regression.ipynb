{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4e8431b-5b14-4ce0-8d52-8eece073a588",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 300\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "SMILES_COL = 'Column3'\n",
    "REGRESSION_COL = 'Column8'\n",
    "URL = '/home/ishii/graduation_research/data/csvファイル/dft_B3LYP_6-31G*_zinc_for-sale_1000000_0to100000.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ca771bf3-36b2-4fec-836d-266ac0d51e60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#regression_col\n",
    "import torch\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, url, smiles_col, regression_col):\n",
    "        self.max_length = 0\n",
    "        self.dummy_char = '_'\n",
    "        \n",
    "        self.url = url\n",
    "        self.smiles_col = smiles_col\n",
    "        self.smiles = []\n",
    "        self.words = self.load_words()\n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "\n",
    "        self.regression_col = regression_col\n",
    "        self.regressions = []\n",
    "        self.items = self.generate_items()\n",
    "        \n",
    "        self.dummmy_index = self.word_to_index[self.dummy_char]\n",
    "\n",
    "    def load_words(self):\n",
    "        train_df = pd.read_csv(self.url, usecols=[SMILES_COL])\n",
    "        self.smiles = list(train_df[self.smiles_col])\n",
    "        for i, smile in enumerate(self.smiles):\n",
    "            new_smile = smile[1:]\n",
    "            self.smiles[i] = new_smile\n",
    "        self.max_length = max(len(smile) for smile in self.smiles)\n",
    "        self.smiles = list(smile.ljust(self.max_length, self.dummy_char) for smile in self.smiles)\n",
    "        train_df = pd.Series(self.smiles)\n",
    "        text = train_df.str.cat(sep=' ')\n",
    "        text = \"\".join(text.split(' '))\n",
    "        return [text[i] for i in range(len(text))]\n",
    "    \n",
    "    def generate_items(self):\n",
    "        train_df = pd.read_csv(self.url, usecols=[REGRESSION_COL])\n",
    "        self.regressions = list(train_df[self.regression_col])\n",
    "        items = []\n",
    "        for i, smile in enumerate(self.smiles):\n",
    "            smile = list(smile)\n",
    "            items.append([self.word_to_index[w] for w in smile])\n",
    "        return items\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.regressions)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.items[index]),\n",
    "            torch.tensor(self.regressions[index])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "281d2fdc-f4d5-4996-a3c7-0a2a265c21a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(url=URL, smiles_col=SMILES_COL, regression_col=REGRESSION_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0267ec8-658a-4e5c-8b58-ba5a20136c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 6,  1,  5,  1,  1,  1,  3,  2, 20,  4,  1,  1,  5,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0]), tensor(5.6882))\n"
     ]
    }
   ],
   "source": [
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd788f74-7bb4-4c23-ab60-18fb3f15af08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87628\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c80b1a8d-0167-41db-b0d3-c46072e6e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ff953cc2-a21b-4620-b448-7d5395224691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6,  1,  5,  1,  1,  1,  1,  3,  6,  4,  1,  5,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  1,  5,  1,  1,  1,  3,  2, 20,  4,  1,  1,  5,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([6.0161, 5.6882])\n",
      "tensor([[ 2,  8,  2,  3,  2, 23,  9,  4,  2,  3,  8,  6,  4,  6,  2,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  5,  1, 10,  1,  3,  2,  4,  1, 10,  5,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([5.7693, 5.4207])\n",
      "tensor([[ 2,  1,  5, 10,  1,  1, 10,  1,  5,  2,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 9,  2,  1,  5,  1,  1,  1, 10,  1,  5,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([5.4346, 5.8732])\n",
      "tensor([[ 2,  1,  5,  1,  1,  3,  6,  4,  1,  1,  1,  5,  2, 20,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  6,  1,  5,  1,  1,  1,  3,  6,  4,  1,  1,  5,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([5.6504, 5.3704])\n",
      "tensor([[ 6,  8,  2,  3,  6,  4,  1,  5,  1,  1,  1, 10,  1,  5,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  8,  2,  3,  6,  4,  1,  5,  1,  1,  1,  1,  1,  5,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([5.5404, 5.8248])\n",
      "tensor([[ 9,  2,  2,  1,  5,  1,  1, 11, 10, 15, 12, 10,  5,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 9,  8,  2,  3,  6,  4,  1,  5,  1, 10,  1,  1, 10,  5,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([7.0071, 4.9437])\n",
      "tensor([[ 2,  2,  5,  3,  2,  4,  2,  6,  2,  3,  9,  4,  8,  9,  5,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 9,  1,  5, 10,  1, 10,  1,  7, 11, 10, 15, 12,  1, 10,  1,  5,  7,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([7.8340, 5.4672])\n",
      "tensor([[ 2,  6,  2,  3,  8,  6,  4,  1,  5,  1,  1,  1, 10,  1,  5,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  8,  2,  3,  6,  4,  1,  5,  1,  1,  1, 11, 10, 24, 12,  3, 11,  6,\n",
      "         17, 12,  4,  1,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([5.5421, 4.2904])\n",
      "tensor([[ 2,  1,  5,  1,  1,  3,  6,  4,  1,  1,  3,  2,  4,  1,  5,  2, 20,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  8,  1,  5, 22,  1,  7,  1,  1,  3,  6,  4,  1,  1,  1,  7, 21,  5,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([5.7927, 5.4066])\n",
      "tensor([[ 6,  2,  1,  5,  1,  1,  1, 10,  1,  5,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  5,  1,  1,  3,  2,  4,  1,  3,  8,  6,  4, 11, 10, 15, 12, 10,\n",
      "          5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([6.1657, 4.7086])\n",
      "tensor([[ 6,  1,  5, 10, 21,  1,  7,  1,  1,  3,  2, 20,  4,  1,  1,  1,  5,  7,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 9,  1,  5,  1,  1,  1,  1,  1,  5,  9,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([4.7690, 5.5853])\n"
     ]
    }
   ],
   "source": [
    "for batch, (x, y) in enumerate(dataloader):\n",
    "    print(x)\n",
    "    print(y)\n",
    "    if batch == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "963eec5a-0677-478b-8f00-91974d4e7ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#変更後のモデル\n",
    "import torch\n",
    "\n",
    "class LSTM_Predictor(torch.nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super(LSTM_Predictor, self).__init__()\n",
    "        self.lstm_size = 128\n",
    "        self.embedding_dim = 128\n",
    "        self.num_layers = 3\n",
    "\n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = torch.nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            #padding_idxの処理が不明確\n",
    "            #padding_idx=dataset.dummmy_index\n",
    "        )\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=self.lstm_size,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(self.lstm_size, 1)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.lstm_size),\n",
    "                torch.zeros(self.num_layers, batch_size, self.lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2cfd0211-c662-47bc-a69c-f76ddb5031c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#変更後の訓練プロセス\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def train(dataset, train_dataset, model):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, drop_last=True)\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    #scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        \n",
    "        state_h, state_c = model.init_state(BATCH_SIZE)\n",
    "        state_h = state_h.to(device)\n",
    "        state_c = state_c.to(device)\n",
    "        total_loss = 0\n",
    "        total_val_loss = 0\n",
    "\n",
    "        for batch, (x, y) in enumerate(train_dataloader):\n",
    "            if batch < int(len(train_dataloader) * 0.75):\n",
    "                model.train()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                y_pred, (state_h, state_c) = model(x.to(device), (state_h, state_c))\n",
    "                y_pred_permute = torch.permute(y_pred, (2, 1, 0))\n",
    "                loss = criterion(y_pred_permute[0][dataset.max_length-1], y.to(device))\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                state_h = state_h.detach()\n",
    "                state_c = state_c.detach()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "                y_pred, (state_h, state_c) = model(x.to(device), (state_h, state_c))\n",
    "                y_pred_permute = torch.permute(y_pred, (2, 1, 0))\n",
    "                val_loss = criterion(y_pred_permute[0, dataset.max_length-1], y.to(device))\n",
    "                total_val_loss += val_loss.item()    \n",
    "                \n",
    "                state_h = state_h.detach()\n",
    "                state_c = state_c.detach()\n",
    "        \n",
    "        #scheduler.step()\n",
    "        \n",
    "        average_total_loss = total_loss / int(len(train_dataloader) * 0.75)\n",
    "        average_total_val_loss = total_val_loss / (len(train_dataloader) - int(len(train_dataloader) * 0.75))\n",
    "        \n",
    "        print(\"Epoch: {}, train_Loss: {}, val_Loss: {}\".format(\n",
    "            epoch+1, \n",
    "            average_total_loss,\n",
    "            average_total_val_loss\n",
    "        ))\n",
    "        losses.append(average_total_loss)\n",
    "        val_losses.append(average_total_val_loss)\n",
    "    return losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a67f788d-627e-42a1-89e3-38510896c476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device: \" + str(device) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3b912c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(url=URL, smiles_col=SMILES_COL, regression_col=REGRESSION_COL)\n",
    "n_samples = len(dataset)\n",
    "indices = list(range(n_samples))\n",
    "train_size = int(n_samples * 0.8)\n",
    "test_size = n_samples - train_size\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, indices[:train_size])\n",
    "test_dataset = torch.utils.data.Subset(dataset, indices[train_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e7f1ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70102\n",
      "17526\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d15e87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410\n",
      "137\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, drop_last=True)\n",
    "print(int(len(train_dataloader) * 0.75))\n",
    "print(len(train_dataloader) - int(len(train_dataloader) * 0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5b9ca541-5e56-4e0f-a42b-5c1b0ec0c701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train_Loss: 0.7132078436816611, val_Loss: 0.4937997934592031\n",
      "Epoch: 2, train_Loss: 0.5676889348320845, val_Loss: 0.4962135081308602\n",
      "Epoch: 3, train_Loss: 0.5678644396909853, val_Loss: 0.4948207836081512\n",
      "Epoch: 4, train_Loss: 0.5680317882357574, val_Loss: 0.4937907220238317\n",
      "Epoch: 5, train_Loss: 0.5681159629327495, val_Loss: 0.49342466140315483\n",
      "Epoch: 6, train_Loss: 0.568184549561361, val_Loss: 0.493562711633905\n",
      "Epoch: 7, train_Loss: 0.5681704861361806, val_Loss: 0.4934414529452359\n",
      "Epoch: 8, train_Loss: 0.5681725446043945, val_Loss: 0.4938330186979614\n",
      "Epoch: 9, train_Loss: 0.5681510332154065, val_Loss: 0.4936944355059714\n",
      "Epoch: 10, train_Loss: 0.5681684610320301, val_Loss: 0.49403783091663445\n",
      "Epoch: 11, train_Loss: 0.5681404861008249, val_Loss: 0.4940595518063455\n",
      "Epoch: 12, train_Loss: 0.5681368502174936, val_Loss: 0.49431280933157373\n",
      "Epoch: 13, train_Loss: 0.568120759507505, val_Loss: 0.49429547590930967\n",
      "Epoch: 14, train_Loss: 0.5681171308930327, val_Loss: 0.49429040646901096\n",
      "Epoch: 15, train_Loss: 0.5681276727013471, val_Loss: 0.49429479567673956\n",
      "Epoch: 16, train_Loss: 0.5681197439751974, val_Loss: 0.49428799161075676\n",
      "Epoch: 17, train_Loss: 0.568125146118606, val_Loss: 0.49429646003855404\n",
      "Epoch: 18, train_Loss: 0.568124593685313, val_Loss: 0.4943002386249765\n",
      "Epoch: 19, train_Loss: 0.5681253129389228, val_Loss: 0.494305844289543\n",
      "Epoch: 20, train_Loss: 0.5681248271610678, val_Loss: 0.49430356734860553\n",
      "Epoch: 21, train_Loss: 0.5681243394933095, val_Loss: 0.49430826458617716\n",
      "Epoch: 22, train_Loss: 0.5681233064430516, val_Loss: 0.49431190046950846\n",
      "Epoch: 23, train_Loss: 0.5681220717546417, val_Loss: 0.4943130364383224\n",
      "Epoch: 24, train_Loss: 0.5681230942650539, val_Loss: 0.494316067573798\n",
      "Epoch: 25, train_Loss: 0.5681229328236929, val_Loss: 0.49431727815718546\n",
      "Epoch: 26, train_Loss: 0.5681226134300232, val_Loss: 0.4943196286250205\n",
      "Epoch: 27, train_Loss: 0.5681225821012403, val_Loss: 0.4943203828195586\n",
      "Epoch: 28, train_Loss: 0.5681220614328617, val_Loss: 0.4943212953797222\n",
      "Epoch: 29, train_Loss: 0.5681224412307506, val_Loss: 0.49432243243621216\n",
      "Epoch: 30, train_Loss: 0.5681226590784585, val_Loss: 0.4943233412982774\n",
      "Epoch: 31, train_Loss: 0.5681224796830154, val_Loss: 0.49432364410727564\n",
      "Epoch: 32, train_Loss: 0.5681222409736819, val_Loss: 0.4943244805301193\n",
      "Epoch: 33, train_Loss: 0.5681226118308742, val_Loss: 0.49432478159883597\n",
      "Epoch: 34, train_Loss: 0.5681224328715627, val_Loss: 0.4943251627205062\n",
      "Epoch: 35, train_Loss: 0.5681223138803388, val_Loss: 0.4943252329843758\n",
      "Epoch: 36, train_Loss: 0.5681222672869519, val_Loss: 0.49432546357168766\n",
      "Epoch: 37, train_Loss: 0.5681223062480368, val_Loss: 0.4943259958803219\n",
      "Epoch: 38, train_Loss: 0.5681223013052126, val_Loss: 0.4943259162624387\n",
      "Epoch: 39, train_Loss: 0.5681223574934936, val_Loss: 0.4943259958803219\n",
      "Epoch: 40, train_Loss: 0.5681222225834683, val_Loss: 0.4943259958803219\n",
      "Epoch: 41, train_Loss: 0.5681222963623884, val_Loss: 0.49432621972404256\n",
      "Epoch: 42, train_Loss: 0.5681222910561212, val_Loss: 0.4943264496587489\n",
      "Epoch: 43, train_Loss: 0.5681222942544193, val_Loss: 0.4943263746091049\n",
      "Epoch: 44, train_Loss: 0.5681223191139174, val_Loss: 0.49432652731881527\n",
      "Epoch: 45, train_Loss: 0.5681222935275334, val_Loss: 0.4943264496587489\n",
      "Epoch: 46, train_Loss: 0.5681222727385963, val_Loss: 0.49432652731881527\n",
      "Epoch: 47, train_Loss: 0.568122277245289, val_Loss: 0.4943265995405016\n",
      "Epoch: 48, train_Loss: 0.5681222588550754, val_Loss: 0.4943265995405016\n",
      "Epoch: 49, train_Loss: 0.5681222443173571, val_Loss: 0.49432652731881527\n",
      "Epoch: 50, train_Loss: 0.5681222659785573, val_Loss: 0.4943265995405016\n",
      "Epoch: 51, train_Loss: 0.5681222698310526, val_Loss: 0.4943265995405016\n",
      "Epoch: 52, train_Loss: 0.5681222771726003, val_Loss: 0.4943264496587489\n",
      "Epoch: 53, train_Loss: 0.5681222565290405, val_Loss: 0.4943265995405016\n",
      "Epoch: 54, train_Loss: 0.5681222933821561, val_Loss: 0.4943265995405016\n",
      "Epoch: 55, train_Loss: 0.5681222745558111, val_Loss: 0.4943264496587489\n",
      "Epoch: 56, train_Loss: 0.5681222481698525, val_Loss: 0.4943265995405016\n",
      "Epoch: 57, train_Loss: 0.5681222812431614, val_Loss: 0.4943266769830328\n",
      "Epoch: 58, train_Loss: 0.5681222936729106, val_Loss: 0.4943266769830328\n",
      "Epoch: 59, train_Loss: 0.5681222827696218, val_Loss: 0.4943266769830328\n",
      "Epoch: 60, train_Loss: 0.5681222860406084, val_Loss: 0.4943264496587489\n",
      "Epoch: 61, train_Loss: 0.5681222661966231, val_Loss: 0.4943265995405016\n",
      "Epoch: 62, train_Loss: 0.5681222731020392, val_Loss: 0.49432675464309916\n",
      "Epoch: 63, train_Loss: 0.5681222894569723, val_Loss: 0.49432675464309916\n",
      "Epoch: 64, train_Loss: 0.568122289311595, val_Loss: 0.4943266769830328\n",
      "Epoch: 65, train_Loss: 0.5681222785536836, val_Loss: 0.49432675464309916\n",
      "Epoch: 66, train_Loss: 0.5681222719390218, val_Loss: 0.4943266769830328\n",
      "Epoch: 67, train_Loss: 0.5681222834238192, val_Loss: 0.4943265995405016\n",
      "Epoch: 68, train_Loss: 0.5681222717936446, val_Loss: 0.4943265995405016\n",
      "Epoch: 69, train_Loss: 0.5681222890208407, val_Loss: 0.4943265995405016\n",
      "Epoch: 70, train_Loss: 0.5681222814612272, val_Loss: 0.4943266769830328\n",
      "Epoch: 71, train_Loss: 0.5681222786263722, val_Loss: 0.4943265995405016\n",
      "Epoch: 72, train_Loss: 0.5681222490421156, val_Loss: 0.4943266769830328\n",
      "Epoch: 73, train_Loss: 0.5681222643067197, val_Loss: 0.4943266769830328\n",
      "Epoch: 74, train_Loss: 0.5681222532580539, val_Loss: 0.4943265995405016\n",
      "Epoch: 75, train_Loss: 0.5681222551479572, val_Loss: 0.4943264496587489\n",
      "Epoch: 76, train_Loss: 0.5681222518769706, val_Loss: 0.49432652731881527\n",
      "Epoch: 77, train_Loss: 0.5681222708486929, val_Loss: 0.4943266769830328\n",
      "Epoch: 78, train_Loss: 0.5681222576193693, val_Loss: 0.4943265995405016\n",
      "Epoch: 79, train_Loss: 0.5681222523131022, val_Loss: 0.49432652731881527\n",
      "Epoch: 80, train_Loss: 0.5681222411190591, val_Loss: 0.4943265995405016\n",
      "Epoch: 81, train_Loss: 0.5681222297069503, val_Loss: 0.4943265995405016\n",
      "Epoch: 82, train_Loss: 0.5681222608903559, val_Loss: 0.4943266769830328\n",
      "Epoch: 83, train_Loss: 0.56812224417198, val_Loss: 0.4943265995405016\n",
      "Epoch: 84, train_Loss: 0.5681222587823868, val_Loss: 0.49432652731881527\n",
      "Epoch: 85, train_Loss: 0.568122266560066, val_Loss: 0.4943265995405016\n",
      "Epoch: 86, train_Loss: 0.5681222793532581, val_Loss: 0.4943266769830328\n",
      "Epoch: 87, train_Loss: 0.5681222691768553, val_Loss: 0.49432675464309916\n",
      "Epoch: 88, train_Loss: 0.5681222507866418, val_Loss: 0.4943264496587489\n",
      "Epoch: 89, train_Loss: 0.5681222453349974, val_Loss: 0.4943264496587489\n",
      "Epoch: 90, train_Loss: 0.5681222334867571, val_Loss: 0.4943265995405016\n",
      "Epoch: 91, train_Loss: 0.5681222705579385, val_Loss: 0.4943266769830328\n",
      "Epoch: 92, train_Loss: 0.5681222859679199, val_Loss: 0.4943266769830328\n",
      "Epoch: 93, train_Loss: 0.5681222807343413, val_Loss: 0.4943265995405016\n",
      "Epoch: 94, train_Loss: 0.5681222776087319, val_Loss: 0.4943266769830328\n",
      "Epoch: 95, train_Loss: 0.5681222650336056, val_Loss: 0.4943265995405016\n",
      "Epoch: 96, train_Loss: 0.5681222707033158, val_Loss: 0.4943266769830328\n",
      "Epoch: 97, train_Loss: 0.5681222695402983, val_Loss: 0.49432652731881527\n",
      "Epoch: 98, train_Loss: 0.5681222570378606, val_Loss: 0.49432652731881527\n",
      "Epoch: 99, train_Loss: 0.5681222547118256, val_Loss: 0.4943265995405016\n",
      "Epoch: 100, train_Loss: 0.5681222684499694, val_Loss: 0.4943265995405016\n",
      "Epoch: 101, train_Loss: 0.5681222670688861, val_Loss: 0.4943266769830328\n",
      "Epoch: 102, train_Loss: 0.5681222707033158, val_Loss: 0.49432675464309916\n",
      "Epoch: 103, train_Loss: 0.5681222662693117, val_Loss: 0.4943266769830328\n",
      "Epoch: 104, train_Loss: 0.5681222686680352, val_Loss: 0.4943266769830328\n",
      "Epoch: 105, train_Loss: 0.5681222600907814, val_Loss: 0.4943266769830328\n",
      "Epoch: 106, train_Loss: 0.5681222706306271, val_Loss: 0.49432652731881527\n",
      "Epoch: 107, train_Loss: 0.5681222806616527, val_Loss: 0.49432652731881527\n",
      "Epoch: 108, train_Loss: 0.5681222513681505, val_Loss: 0.49432652731881527\n",
      "Epoch: 109, train_Loss: 0.5681222824788675, val_Loss: 0.4943265995405016\n",
      "Epoch: 110, train_Loss: 0.5681222737562366, val_Loss: 0.4943266769830328\n",
      "Epoch: 111, train_Loss: 0.5681222701944956, val_Loss: 0.4943266769830328\n",
      "Epoch: 112, train_Loss: 0.5681222794986353, val_Loss: 0.49432675464309916\n",
      "Epoch: 113, train_Loss: 0.5681222757915171, val_Loss: 0.4943265995405016\n",
      "Epoch: 114, train_Loss: 0.5681222716482675, val_Loss: 0.4943266769830328\n",
      "Epoch: 115, train_Loss: 0.5681222901838582, val_Loss: 0.4943265995405016\n",
      "Epoch: 116, train_Loss: 0.5681223066114798, val_Loss: 0.4943266769830328\n",
      "Epoch: 117, train_Loss: 0.5681222871309374, val_Loss: 0.4943265995405016\n",
      "Epoch: 118, train_Loss: 0.5681222834965077, val_Loss: 0.4943266769830328\n",
      "Epoch: 119, train_Loss: 0.568122255729466, val_Loss: 0.49432675464309916\n",
      "Epoch: 120, train_Loss: 0.5681222799347668, val_Loss: 0.4943266769830328\n",
      "Epoch: 121, train_Loss: 0.5681222779721748, val_Loss: 0.4943266769830328\n",
      "Epoch: 122, train_Loss: 0.5681222679411493, val_Loss: 0.4943266769830328\n",
      "Epoch: 123, train_Loss: 0.5681222626348821, val_Loss: 0.49432652731881527\n",
      "Epoch: 124, train_Loss: 0.5681222747011883, val_Loss: 0.4943265995405016\n",
      "Epoch: 125, train_Loss: 0.5681222817519817, val_Loss: 0.4943265995405016\n",
      "Epoch: 126, train_Loss: 0.5681222779721748, val_Loss: 0.4943265995405016\n",
      "Epoch: 127, train_Loss: 0.568122279716701, val_Loss: 0.4943266769830328\n",
      "Epoch: 128, train_Loss: 0.5681222793532581, val_Loss: 0.49432675464309916\n",
      "Epoch: 129, train_Loss: 0.5681222640159653, val_Loss: 0.4943265995405016\n",
      "Epoch: 130, train_Loss: 0.5681222578374351, val_Loss: 0.4943265995405016\n",
      "Epoch: 131, train_Loss: 0.56812224984169, val_Loss: 0.4943266769830328\n",
      "Epoch: 132, train_Loss: 0.5681222552933344, val_Loss: 0.4943266769830328\n",
      "Epoch: 133, train_Loss: 0.5681222570378606, val_Loss: 0.49432652731881527\n",
      "Epoch: 134, train_Loss: 0.5681222478064095, val_Loss: 0.4943266769830328\n",
      "Epoch: 135, train_Loss: 0.5681222456257518, val_Loss: 0.4943265995405016\n",
      "Epoch: 136, train_Loss: 0.5681222428635853, val_Loss: 0.49432652731881527\n",
      "Epoch: 137, train_Loss: 0.5681222508593303, val_Loss: 0.4943265995405016\n",
      "Epoch: 138, train_Loss: 0.5681222478064095, val_Loss: 0.4943266769830328\n",
      "Epoch: 139, train_Loss: 0.5681222523131022, val_Loss: 0.4943265995405016\n",
      "Epoch: 140, train_Loss: 0.5681222474429666, val_Loss: 0.49432675464309916\n",
      "Epoch: 141, train_Loss: 0.5681222531853652, val_Loss: 0.4943265995405016\n",
      "Epoch: 142, train_Loss: 0.5681222478064095, val_Loss: 0.4943264496587489\n",
      "Epoch: 143, train_Loss: 0.5681222390110899, val_Loss: 0.4943266769830328\n",
      "Epoch: 144, train_Loss: 0.5681222555840888, val_Loss: 0.4943265995405016\n",
      "Epoch: 145, train_Loss: 0.5681222668508205, val_Loss: 0.49432652731881527\n",
      "Epoch: 146, train_Loss: 0.5681222653243599, val_Loss: 0.4943265995405016\n",
      "Epoch: 147, train_Loss: 0.568122260526913, val_Loss: 0.4943265995405016\n",
      "Epoch: 148, train_Loss: 0.5681222605996016, val_Loss: 0.4943266769830328\n",
      "Epoch: 149, train_Loss: 0.5681222561655975, val_Loss: 0.4943265995405016\n",
      "Epoch: 150, train_Loss: 0.5681222591458297, val_Loss: 0.4943264496587489\n",
      "Epoch: 151, train_Loss: 0.5681222494782471, val_Loss: 0.49432652731881527\n",
      "Epoch: 152, train_Loss: 0.5681222676503949, val_Loss: 0.49432652731881527\n",
      "Epoch: 153, train_Loss: 0.5681222683045922, val_Loss: 0.4943266769830328\n",
      "Epoch: 154, train_Loss: 0.568122286840183, val_Loss: 0.4943266769830328\n",
      "Epoch: 155, train_Loss: 0.5681222769545345, val_Loss: 0.4943265995405016\n",
      "Epoch: 156, train_Loss: 0.5681222706306271, val_Loss: 0.4943265995405016\n",
      "Epoch: 157, train_Loss: 0.5681222663420002, val_Loss: 0.49432652731881527\n",
      "Epoch: 158, train_Loss: 0.5681222602361586, val_Loss: 0.4943265995405016\n",
      "Epoch: 159, train_Loss: 0.5681222880032004, val_Loss: 0.4943266769830328\n",
      "Epoch: 160, train_Loss: 0.56812227615496, val_Loss: 0.4943266769830328\n",
      "Epoch: 161, train_Loss: 0.5681222729566621, val_Loss: 0.4943266769830328\n",
      "Epoch: 162, train_Loss: 0.5681222794986353, val_Loss: 0.4943266769830328\n",
      "Epoch: 163, train_Loss: 0.568122277245289, val_Loss: 0.4943265995405016\n",
      "Epoch: 164, train_Loss: 0.5681222458438175, val_Loss: 0.4943265995405016\n",
      "Epoch: 165, train_Loss: 0.5681222555114002, val_Loss: 0.4943265995405016\n",
      "Epoch: 166, train_Loss: 0.5681222850956568, val_Loss: 0.4943266769830328\n",
      "Epoch: 167, train_Loss: 0.5681222844414595, val_Loss: 0.49432652731881527\n",
      "Epoch: 168, train_Loss: 0.5681222556567773, val_Loss: 0.4943266769830328\n",
      "Epoch: 169, train_Loss: 0.5681222792078809, val_Loss: 0.49432675464309916\n",
      "Epoch: 170, train_Loss: 0.5681222901838582, val_Loss: 0.4943266769830328\n",
      "Epoch: 171, train_Loss: 0.5681222587096982, val_Loss: 0.4943266769830328\n",
      "Epoch: 172, train_Loss: 0.5681222566744176, val_Loss: 0.4943265995405016\n",
      "Epoch: 173, train_Loss: 0.5681222646701627, val_Loss: 0.4943265995405016\n",
      "Epoch: 174, train_Loss: 0.5681222692495439, val_Loss: 0.49432652731881527\n",
      "Epoch: 175, train_Loss: 0.5681222567471063, val_Loss: 0.49432652731881527\n",
      "Epoch: 176, train_Loss: 0.5681222824788675, val_Loss: 0.49432675464309916\n",
      "Epoch: 177, train_Loss: 0.5681222978161602, val_Loss: 0.49432675464309916\n",
      "Epoch: 178, train_Loss: 0.5681222732474164, val_Loss: 0.49432675464309916\n",
      "Epoch: 179, train_Loss: 0.5681222642340311, val_Loss: 0.4943266769830328\n",
      "Epoch: 180, train_Loss: 0.5681222773179775, val_Loss: 0.49432652731881527\n",
      "Epoch: 181, train_Loss: 0.5681222581281895, val_Loss: 0.4943265995405016\n",
      "Epoch: 182, train_Loss: 0.5681222879305119, val_Loss: 0.4943266769830328\n",
      "Epoch: 183, train_Loss: 0.5681222708486929, val_Loss: 0.4943266769830328\n",
      "Epoch: 184, train_Loss: 0.5681222712848245, val_Loss: 0.4943265995405016\n",
      "Epoch: 185, train_Loss: 0.5681222663420002, val_Loss: 0.4943265995405016\n",
      "Epoch: 186, train_Loss: 0.5681222547118256, val_Loss: 0.4943265995405016\n",
      "Epoch: 187, train_Loss: 0.5681222796440124, val_Loss: 0.49432675464309916\n",
      "Epoch: 188, train_Loss: 0.5681222499870673, val_Loss: 0.49432652731881527\n",
      "Epoch: 189, train_Loss: 0.5681222462072605, val_Loss: 0.49432652731881527\n",
      "Epoch: 190, train_Loss: 0.5681222559475317, val_Loss: 0.4943265995405016\n",
      "Epoch: 191, train_Loss: 0.5681222480244753, val_Loss: 0.4943266769830328\n",
      "Epoch: 192, train_Loss: 0.5681222558021546, val_Loss: 0.4943265995405016\n",
      "Epoch: 193, train_Loss: 0.5681222549298914, val_Loss: 0.4943266769830328\n",
      "Epoch: 194, train_Loss: 0.5681222801528326, val_Loss: 0.4943266769830328\n",
      "Epoch: 195, train_Loss: 0.5681222555840888, val_Loss: 0.4943265995405016\n",
      "Epoch: 196, train_Loss: 0.5681222616172419, val_Loss: 0.49432652731881527\n",
      "Epoch: 197, train_Loss: 0.5681222387930241, val_Loss: 0.49432652731881527\n",
      "Epoch: 198, train_Loss: 0.5681222456257518, val_Loss: 0.4943265995405016\n",
      "Epoch: 199, train_Loss: 0.5681222452623088, val_Loss: 0.4943265995405016\n",
      "Epoch: 200, train_Loss: 0.568122245044243, val_Loss: 0.4943266769830328\n",
      "Epoch: 201, train_Loss: 0.5681222523857907, val_Loss: 0.4943266769830328\n",
      "Epoch: 202, train_Loss: 0.5681222580555009, val_Loss: 0.4943265995405016\n",
      "Epoch: 203, train_Loss: 0.5681222476610324, val_Loss: 0.4943264496587489\n",
      "Epoch: 204, train_Loss: 0.5681222595092726, val_Loss: 0.4943263746091049\n",
      "Epoch: 205, train_Loss: 0.5681222269447839, val_Loss: 0.4943265995405016\n",
      "Epoch: 206, train_Loss: 0.5681222717936446, val_Loss: 0.4943266769830328\n",
      "Epoch: 207, train_Loss: 0.5681222743377453, val_Loss: 0.4943266769830328\n",
      "Epoch: 208, train_Loss: 0.5681222666327547, val_Loss: 0.4943265995405016\n",
      "Epoch: 209, train_Loss: 0.5681222754280741, val_Loss: 0.4943266769830328\n",
      "Epoch: 210, train_Loss: 0.5681222584189438, val_Loss: 0.4943266769830328\n",
      "Epoch: 211, train_Loss: 0.5681222745558111, val_Loss: 0.4943266769830328\n",
      "Epoch: 212, train_Loss: 0.5681222676503949, val_Loss: 0.49432652731881527\n",
      "Epoch: 213, train_Loss: 0.5681222559475317, val_Loss: 0.4943265995405016\n",
      "Epoch: 214, train_Loss: 0.5681222511500847, val_Loss: 0.4943265995405016\n",
      "Epoch: 215, train_Loss: 0.5681222892389065, val_Loss: 0.4943266769830328\n",
      "Epoch: 216, train_Loss: 0.5681222990518663, val_Loss: 0.4943265995405016\n",
      "Epoch: 217, train_Loss: 0.5681222552933344, val_Loss: 0.4943265995405016\n",
      "Epoch: 218, train_Loss: 0.5681222675050177, val_Loss: 0.4943266769830328\n",
      "Epoch: 219, train_Loss: 0.5681222699037413, val_Loss: 0.49432675464309916\n",
      "Epoch: 220, train_Loss: 0.5681222619079962, val_Loss: 0.4943265995405016\n",
      "Epoch: 221, train_Loss: 0.5681222712121359, val_Loss: 0.49432652731881527\n",
      "Epoch: 222, train_Loss: 0.5681222624168163, val_Loss: 0.4943265995405016\n",
      "Epoch: 223, train_Loss: 0.5681222746284996, val_Loss: 0.4943265995405016\n",
      "Epoch: 224, train_Loss: 0.5681222834238192, val_Loss: 0.4943265995405016\n",
      "Epoch: 225, train_Loss: 0.5681222811704729, val_Loss: 0.4943266769830328\n",
      "Epoch: 226, train_Loss: 0.5681222865494286, val_Loss: 0.49432675464309916\n",
      "Epoch: 227, train_Loss: 0.5681222926552703, val_Loss: 0.4943266769830328\n",
      "Epoch: 228, train_Loss: 0.5681222616172419, val_Loss: 0.4943266769830328\n",
      "Epoch: 229, train_Loss: 0.568122278480995, val_Loss: 0.4943266769830328\n",
      "Epoch: 230, train_Loss: 0.5681222651062943, val_Loss: 0.4943266769830328\n",
      "Epoch: 231, train_Loss: 0.5681222975980945, val_Loss: 0.4943266769830328\n",
      "Epoch: 232, train_Loss: 0.56812228647674, val_Loss: 0.4943266769830328\n",
      "Epoch: 233, train_Loss: 0.5681222824061789, val_Loss: 0.4943266769830328\n",
      "Epoch: 234, train_Loss: 0.5681222756461399, val_Loss: 0.4943266769830328\n",
      "Epoch: 235, train_Loss: 0.5681222781902406, val_Loss: 0.4943266769830328\n",
      "Epoch: 236, train_Loss: 0.5681222801528326, val_Loss: 0.4943266769830328\n",
      "Epoch: 237, train_Loss: 0.5681222817519817, val_Loss: 0.4943266769830328\n",
      "Epoch: 238, train_Loss: 0.568122286840183, val_Loss: 0.49432675464309916\n",
      "Epoch: 239, train_Loss: 0.5681222790625037, val_Loss: 0.4943265995405016\n",
      "Epoch: 240, train_Loss: 0.5681222668508205, val_Loss: 0.49432675464309916\n",
      "Epoch: 241, train_Loss: 0.5681222731747279, val_Loss: 0.49432675464309916\n",
      "Epoch: 242, train_Loss: 0.5681222576920579, val_Loss: 0.4943265995405016\n",
      "Epoch: 243, train_Loss: 0.5681222482425411, val_Loss: 0.49432652731881527\n",
      "Epoch: 244, train_Loss: 0.568122257328615, val_Loss: 0.4943265995405016\n",
      "Epoch: 245, train_Loss: 0.5681222737562366, val_Loss: 0.4943265995405016\n",
      "Epoch: 246, train_Loss: 0.568122272447842, val_Loss: 0.4943265995405016\n",
      "Epoch: 247, train_Loss: 0.5681222718663332, val_Loss: 0.4943266769830328\n",
      "Epoch: 248, train_Loss: 0.5681222807343413, val_Loss: 0.4943266769830328\n",
      "Epoch: 249, train_Loss: 0.5681222555114002, val_Loss: 0.4943265995405016\n",
      "Epoch: 250, train_Loss: 0.5681222371211866, val_Loss: 0.4943265995405016\n",
      "Epoch: 251, train_Loss: 0.5681222392291557, val_Loss: 0.49432652731881527\n",
      "Epoch: 252, train_Loss: 0.568122247006835, val_Loss: 0.4943265995405016\n",
      "Epoch: 253, train_Loss: 0.5681222350859061, val_Loss: 0.49432652731881527\n",
      "Epoch: 254, train_Loss: 0.5681222587823868, val_Loss: 0.4943265995405016\n",
      "Epoch: 255, train_Loss: 0.5681222593638955, val_Loss: 0.4943265995405016\n",
      "Epoch: 256, train_Loss: 0.5681222530399881, val_Loss: 0.4943265995405016\n",
      "Epoch: 257, train_Loss: 0.568122259291207, val_Loss: 0.49432675464309916\n",
      "Epoch: 258, train_Loss: 0.5681222456257518, val_Loss: 0.4943265995405016\n",
      "Epoch: 259, train_Loss: 0.5681222385022698, val_Loss: 0.4943265995405016\n",
      "Epoch: 260, train_Loss: 0.5681222531853652, val_Loss: 0.49432675464309916\n",
      "Epoch: 261, train_Loss: 0.5681222604542244, val_Loss: 0.49432652731881527\n",
      "Epoch: 262, train_Loss: 0.5681222690314781, val_Loss: 0.4943265995405016\n",
      "Epoch: 263, train_Loss: 0.5681222744104338, val_Loss: 0.4943265995405016\n",
      "Epoch: 264, train_Loss: 0.5681222546391371, val_Loss: 0.4943265995405016\n",
      "Epoch: 265, train_Loss: 0.5681222682319036, val_Loss: 0.4943265995405016\n",
      "Epoch: 266, train_Loss: 0.568122266923509, val_Loss: 0.4943265995405016\n",
      "Epoch: 267, train_Loss: 0.568122266923509, val_Loss: 0.4943265995405016\n",
      "Epoch: 268, train_Loss: 0.5681222725932191, val_Loss: 0.4943266769830328\n",
      "Epoch: 269, train_Loss: 0.5681222825515561, val_Loss: 0.49432652731881527\n",
      "Epoch: 270, train_Loss: 0.5681222742650567, val_Loss: 0.4943265995405016\n",
      "Epoch: 271, train_Loss: 0.5681222664146889, val_Loss: 0.4943265995405016\n",
      "Epoch: 272, train_Loss: 0.5681222957081911, val_Loss: 0.4943266769830328\n",
      "Epoch: 273, train_Loss: 0.5681222687407238, val_Loss: 0.4943266769830328\n",
      "Epoch: 274, train_Loss: 0.5681222673596406, val_Loss: 0.4943266769830328\n",
      "Epoch: 275, train_Loss: 0.5681222663420002, val_Loss: 0.4943265995405016\n",
      "Epoch: 276, train_Loss: 0.5681222686680352, val_Loss: 0.4943264496587489\n",
      "Epoch: 277, train_Loss: 0.5681222488240498, val_Loss: 0.4943265995405016\n",
      "Epoch: 278, train_Loss: 0.5681222784083064, val_Loss: 0.4943265995405016\n",
      "Epoch: 279, train_Loss: 0.5681222805162756, val_Loss: 0.49432652731881527\n",
      "Epoch: 280, train_Loss: 0.568122270121807, val_Loss: 0.4943265995405016\n",
      "Epoch: 281, train_Loss: 0.5681222742650567, val_Loss: 0.49432652731881527\n",
      "Epoch: 282, train_Loss: 0.5681222898204152, val_Loss: 0.4943266769830328\n",
      "Epoch: 283, train_Loss: 0.5681222847322138, val_Loss: 0.4943265995405016\n",
      "Epoch: 284, train_Loss: 0.5681222876397575, val_Loss: 0.4943266769830328\n",
      "Epoch: 285, train_Loss: 0.5681222872036259, val_Loss: 0.49432682642971515\n",
      "Epoch: 286, train_Loss: 0.5681223139530275, val_Loss: 0.49432675464309916\n",
      "Epoch: 287, train_Loss: 0.5681223008690811, val_Loss: 0.49432675464309916\n",
      "Epoch: 288, train_Loss: 0.5681222840780165, val_Loss: 0.4943266769830328\n",
      "Epoch: 289, train_Loss: 0.568122284877591, val_Loss: 0.49432652731881527\n",
      "Epoch: 290, train_Loss: 0.5681222776814205, val_Loss: 0.4943266769830328\n",
      "Epoch: 291, train_Loss: 0.5681222664873774, val_Loss: 0.4943266769830328\n",
      "Epoch: 292, train_Loss: 0.5681222789898152, val_Loss: 0.4943266769830328\n",
      "Epoch: 293, train_Loss: 0.5681222835691964, val_Loss: 0.4943266769830328\n",
      "Epoch: 294, train_Loss: 0.5681222985430462, val_Loss: 0.49432675464309916\n",
      "Epoch: 295, train_Loss: 0.568122278844438, val_Loss: 0.49432675464309916\n",
      "Epoch: 296, train_Loss: 0.5681222720117104, val_Loss: 0.49432675464309916\n",
      "Epoch: 297, train_Loss: 0.5681222808797185, val_Loss: 0.4943266769830328\n",
      "Epoch: 298, train_Loss: 0.5681222777541091, val_Loss: 0.4943266769830328\n",
      "Epoch: 299, train_Loss: 0.5681222827696218, val_Loss: 0.4943266769830328\n",
      "Epoch: 300, train_Loss: 0.5681222824061789, val_Loss: 0.4943265995405016\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_Predictor(dataset)\n",
    "model = model.to(device)\n",
    "train_losses, val_losses = train(dataset, train_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27a7152e-3438-43f2-9432-f1b189df9404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predect(dataset, test_dataset, model):\n",
    "    model.eval()\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=5, drop_last=True)\n",
    "\n",
    "    index = np.random.choice(len(test_dataloader))\n",
    "    \n",
    "    for batch, (x, y) in enumerate(test_dataloader):\n",
    "    \n",
    "        smiles, wavelength = x, y\n",
    "        break\n",
    "    state_h, state_c = model.init_state(5)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    y_pred, (state_h, state_c) = model(smiles.to(device), (state_h, state_c))\n",
    "    y_pred_permute = torch.permute(y_pred, (2, 1, 0))    \n",
    "\n",
    "    print(y_pred_permute)\n",
    "    print(wavelength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "df41f473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1847, 0.0633, 0.0633, 0.0633, 0.0633],\n",
      "         [0.8059, 1.2008, 0.3606, 1.2008, 1.2008],\n",
      "         [2.3484, 3.6352, 1.9047, 3.6352, 3.6034],\n",
      "         [3.8402, 4.4095, 3.8427, 4.4095, 4.4003],\n",
      "         [4.4072, 4.5534, 4.4189, 4.5534, 4.5517],\n",
      "         [4.5477, 4.5778, 4.5509, 4.5778, 4.5775],\n",
      "         [4.5762, 4.5821, 4.5768, 4.5821, 4.5820],\n",
      "         [4.5817, 4.5830, 4.5818, 4.5830, 4.5829],\n",
      "         [4.5828, 4.5832, 4.5829, 4.5832, 4.5831],\n",
      "         [4.5831, 4.5832, 4.5831, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832],\n",
      "         [4.5832, 4.5832, 4.5832, 4.5832, 4.5832]]], device='cuda:0',\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "tensor([4.4789, 5.0128, 4.6577, 5.1543, 3.6800])\n"
     ]
    }
   ],
   "source": [
    "predect(dataset, test_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cc12867b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\n",
    "axes[0].plot(train_losses, label=\"train_Loss\")\n",
    "axes[0].plot(val_losses, label=\"val_Loss\")\n",
    "axes[0].grid()\n",
    "axes[0].set_xlabel(\"epoch\")\n",
    "axes[0].legend()\n",
    "\n",
    "\n",
    "axes[1].plot(train_losses, label=\"train_Loss\")\n",
    "axes[1].plot(val_losses, label=\"val_Loss\")\n",
    "axes[1].grid()\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].set_xlabel(\"epoch\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "91fdc21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataset, test_dataset, model):\n",
    "    model.eval()\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    batch_size = 1\n",
    "    \n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, drop_last=True)\n",
    "    \n",
    "    criterion = torch.nn.L1Loss()\n",
    "    \n",
    "    total_loss = 0\n",
    "    state_h, state_c = model.init_state(batch_size)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    \n",
    "    for batch, (x, y) in enumerate(test_dataloader):\n",
    "    \n",
    "        y_pred, (state_h, state_c) = model(x.to(device), (state_h, state_c))\n",
    "        y_pred_permute = torch.permute(y_pred, (2, 1, 0))    \n",
    "        loss = criterion(y_pred_permute[0, dataset.max_length-1], y.to(device))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        state_h = state_h.detach()\n",
    "        state_c = state_c.detach()\n",
    "    \n",
    "    average_total_loss = total_loss / len(test_dataloader)\n",
    "    \n",
    "    return average_total_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce14c760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_Loss: 0.654\n"
     ]
    }
   ],
   "source": [
    "print(\"test_Loss: {:.3f}\".format(test(dataset, test_dataset, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "64cfc140",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 22.42 GiB (GPU 0; 23.70 GiB total capacity; 16.85 GiB already allocated; 5.01 GiB free; 17.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [63], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m state_c \u001b[39m=\u001b[39m state_c\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m batch, (x, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(test_dataloader):\n\u001b[0;32m---> 14\u001b[0m     y_pred, (state_h, state_c) \u001b[39m=\u001b[39m model(x\u001b[39m.\u001b[39;49mto(device), (state_h, state_c))\n\u001b[1;32m     15\u001b[0m     y_pred_permute \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mpermute(y_pred, (\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m))    \n\u001b[1;32m     16\u001b[0m     state_h \u001b[39m=\u001b[39m state_h\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/mambaforge/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [51], line 29\u001b[0m, in \u001b[0;36mLSTM_Predictor.forward\u001b[0;34m(self, x, prev_state)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, prev_state):\n\u001b[1;32m     28\u001b[0m     embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[0;32m---> 29\u001b[0m     output, state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(embed, prev_state)\n\u001b[1;32m     30\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(output)\n\u001b[1;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m logits, state\n",
      "File \u001b[0;32m~/mambaforge/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/rnn.py:769\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    770\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    771\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    773\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 22.42 GiB (GPU 0; 23.70 GiB total capacity; 16.85 GiB already allocated; 5.01 GiB free; 17.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = len(test_dataset)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "state_h, state_c = model.init_state(batch_size)\n",
    "state_h = state_h.to(device)\n",
    "state_c = state_c.to(device)\n",
    "\n",
    "for batch, (x, y) in enumerate(test_dataloader):\n",
    "\n",
    "    y_pred, (state_h, state_c) = model(x.to(device), (state_h, state_c))\n",
    "    y_pred_permute = torch.permute(y_pred, (2, 1, 0))    \n",
    "    state_h = state_h.detach()\n",
    "    state_c = state_c.detach()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(y.cpu().detach().numpy(), y_pred_permute[0, dataset.max_length-1].cpu().detach().numpy())\n",
    "plt.title(REGRESSION_COL)\n",
    "plt.xlabel(\"labels\")\n",
    "plt.ylabel(\"output\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f18a3795fe5cb6a973482da1273789e92c21cec72867792a35f0dcfcd5bea518"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
