{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e8431b-5b14-4ce0-8d52-8eece073a588",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 300\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "SMILES_COL = 'Column3'\n",
    "REGRESSION_COL = 'Column8'\n",
    "URL = '/home/ishii/graduation_research/data/csvファイル/dft_B3LYP_6-31G*_zinc_for-sale_1000000_0to100000.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca771bf3-36b2-4fec-836d-266ac0d51e60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#regression_col\n",
    "import torch\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, url, smiles_col, regression_col):\n",
    "        self.max_length = 0\n",
    "        self.dummy_char = '_'\n",
    "        \n",
    "        self.url = url\n",
    "        self.smiles_col = smiles_col\n",
    "        self.smiles = []\n",
    "        self.words = self.load_words()\n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "\n",
    "        self.regression_col = regression_col\n",
    "        self.regressions = []\n",
    "        self.items = self.generate_items()\n",
    "        \n",
    "        self.dummmy_index = self.word_to_index[self.dummy_char]\n",
    "\n",
    "    def load_words(self):\n",
    "        train_df = pd.read_csv(self.url, usecols=[SMILES_COL])\n",
    "        self.smiles = list(train_df[self.smiles_col])\n",
    "        for i, smile in enumerate(self.smiles):\n",
    "            new_smile = smile[1:]\n",
    "            self.smiles[i] = new_smile\n",
    "        self.max_length = max(len(smile) for smile in self.smiles)\n",
    "        self.smiles = list(smile.ljust(self.max_length, self.dummy_char) for smile in self.smiles)\n",
    "        train_df = pd.Series(self.smiles)\n",
    "        text = train_df.str.cat(sep=' ')\n",
    "        text = \"\".join(text.split(' '))\n",
    "        return [text[i] for i in range(len(text))]\n",
    "    \n",
    "    def generate_items(self):\n",
    "        train_df = pd.read_csv(self.url, usecols=[REGRESSION_COL])\n",
    "        self.regressions = list(train_df[self.regression_col])\n",
    "        items = []\n",
    "        for i, smile in enumerate(self.smiles):\n",
    "            smile = list(smile)\n",
    "            items.append([self.word_to_index[w] for w in smile])\n",
    "        return items\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.regressions)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.items[index]),\n",
    "            torch.tensor(self.regressions[index])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "281d2fdc-f4d5-4996-a3c7-0a2a265c21a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(url=URL, smiles_col=SMILES_COL, regression_col=REGRESSION_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0267ec8-658a-4e5c-8b58-ba5a20136c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 6,  1,  5,  1,  1,  1,  3,  2, 20,  4,  1,  1,  5,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0]), tensor(5.6882))\n"
     ]
    }
   ],
   "source": [
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd788f74-7bb4-4c23-ab60-18fb3f15af08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87628\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c80b1a8d-0167-41db-b0d3-c46072e6e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff953cc2-a21b-4620-b448-7d5395224691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6,  1,  5,  1,  1,  1,  1,  3,  6,  4,  1,  5,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  1,  5,  1,  1,  1,  3,  2, 20,  4,  1,  1,  5,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([6.0161, 5.6882])\n",
      "tensor([[ 2,  8,  2,  3,  2, 23,  9,  4,  2,  3,  8,  6,  4,  6,  2,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  5,  1, 10,  1,  3,  2,  4,  1, 10,  5,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([5.7693, 5.4207])\n",
      "tensor([[ 2,  1,  5, 10,  1,  1, 10,  1,  5,  2,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 9,  2,  1,  5,  1,  1,  1, 10,  1,  5,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([5.4346, 5.8732])\n",
      "tensor([[ 2,  1,  5,  1,  1,  3,  6,  4,  1,  1,  1,  5,  2, 20,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  6,  1,  5,  1,  1,  1,  3,  6,  4,  1,  1,  5,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([5.6504, 5.3704])\n",
      "tensor([[ 6,  8,  2,  3,  6,  4,  1,  5,  1,  1,  1, 10,  1,  5,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  8,  2,  3,  6,  4,  1,  5,  1,  1,  1,  1,  1,  5,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([5.5404, 5.8248])\n",
      "tensor([[ 9,  2,  2,  1,  5,  1,  1, 11, 10, 15, 12, 10,  5,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 9,  8,  2,  3,  6,  4,  1,  5,  1, 10,  1,  1, 10,  5,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([7.0071, 4.9437])\n",
      "tensor([[ 2,  2,  5,  3,  2,  4,  2,  6,  2,  3,  9,  4,  8,  9,  5,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 9,  1,  5, 10,  1, 10,  1,  7, 11, 10, 15, 12,  1, 10,  1,  5,  7,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([7.8340, 5.4672])\n",
      "tensor([[ 2,  6,  2,  3,  8,  6,  4,  1,  5,  1,  1,  1, 10,  1,  5,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  8,  2,  3,  6,  4,  1,  5,  1,  1,  1, 11, 10, 24, 12,  3, 11,  6,\n",
      "         17, 12,  4,  1,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([5.5421, 4.2904])\n",
      "tensor([[ 2,  1,  5,  1,  1,  3,  6,  4,  1,  1,  3,  2,  4,  1,  5,  2, 20,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  8,  1,  5, 22,  1,  7,  1,  1,  3,  6,  4,  1,  1,  1,  7, 21,  5,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([5.7927, 5.4066])\n",
      "tensor([[ 6,  2,  1,  5,  1,  1,  1, 10,  1,  5,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  5,  1,  1,  3,  2,  4,  1,  3,  8,  6,  4, 11, 10, 15, 12, 10,\n",
      "          5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([6.1657, 4.7086])\n",
      "tensor([[ 6,  1,  5, 10, 21,  1,  7,  1,  1,  3,  2, 20,  4,  1,  1,  1,  5,  7,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 9,  1,  5,  1,  1,  1,  1,  1,  5,  9,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([4.7690, 5.5853])\n"
     ]
    }
   ],
   "source": [
    "for batch, (x, y) in enumerate(dataloader):\n",
    "    print(x)\n",
    "    print(y)\n",
    "    if batch == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "963eec5a-0677-478b-8f00-91974d4e7ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#変更後のモデル\n",
    "import torch\n",
    "\n",
    "class LSTM_Predictor(torch.nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super(LSTM_Predictor, self).__init__()\n",
    "        self.lstm_size = 256\n",
    "        self.embedding_dim = 256\n",
    "        self.num_layers = 3\n",
    "\n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = torch.nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            #padding_idxの処理が不明確\n",
    "            #padding_idx=dataset.dummmy_index\n",
    "        )\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=self.lstm_size,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(self.lstm_size, 1)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.lstm_size),\n",
    "                torch.zeros(self.num_layers, batch_size, self.lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cfd0211-c662-47bc-a69c-f76ddb5031c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#変更後の訓練プロセス\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def train(dataset, train_dataset, model):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, drop_last=True)\n",
    "\n",
    "    criterion = torch.nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    #scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        \n",
    "        state_h, state_c = model.init_state(BATCH_SIZE)\n",
    "        state_h = state_h.to(device)\n",
    "        state_c = state_c.to(device)\n",
    "        total_loss = 0\n",
    "        total_val_loss = 0\n",
    "\n",
    "        for batch, (x, y) in enumerate(train_dataloader):\n",
    "            if batch < int(len(train_dataloader) * 0.75):\n",
    "                model.train()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                y_pred, (state_h, state_c) = model(x.to(device), (state_h, state_c))\n",
    "                y_pred_permute = torch.permute(y_pred, (2, 1, 0))\n",
    "                loss = criterion(y_pred_permute[0][dataset.max_length-1], y.to(device))\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                state_h = state_h.detach()\n",
    "                state_c = state_c.detach()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "                y_pred, (state_h, state_c) = model(x.to(device), (state_h, state_c))\n",
    "                y_pred_permute = torch.permute(y_pred, (2, 1, 0))\n",
    "                val_loss = criterion(y_pred_permute[0, dataset.max_length-1], y.to(device))\n",
    "                total_val_loss += val_loss.item()    \n",
    "                \n",
    "                state_h = state_h.detach()\n",
    "                state_c = state_c.detach()\n",
    "        \n",
    "        #scheduler.step()\n",
    "        \n",
    "        print(\"Epoch: {}, train_Loss: {:.3f}, val_Loss: {:.3f}\".format(\n",
    "            epoch+1, \n",
    "            total_loss / int(len(train_dataloader) * 0.75),\n",
    "            total_val_loss / (len(train_dataloader) - int(len(train_dataloader) * 0.75))\n",
    "        ))\n",
    "        losses.append(total_loss)\n",
    "        val_losses.append(total_val_loss)\n",
    "    return losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a67f788d-627e-42a1-89e3-38510896c476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device: \" + str(device) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b912c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(url=URL, smiles_col=SMILES_COL, regression_col=REGRESSION_COL)\n",
    "n_samples = len(dataset)\n",
    "indices = list(range(n_samples))\n",
    "train_size = int(n_samples * 0.8)\n",
    "test_size = n_samples - train_size\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, indices[:train_size])\n",
    "test_dataset = torch.utils.data.Subset(dataset, indices[train_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e7f1ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70102\n",
      "17526\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d15e87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410\n",
      "137\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, drop_last=True)\n",
    "print(int(len(train_dataloader) * 0.75))\n",
    "print(len(train_dataloader) - int(len(train_dataloader) * 0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b9ca541-5e56-4e0f-a42b-5c1b0ec0c701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train_Loss: 1.842, val_Loss: 1.746\n",
      "Epoch: 2, train_Loss: 1.792, val_Loss: 1.722\n",
      "Epoch: 3, train_Loss: 1.792, val_Loss: 1.718\n",
      "Epoch: 4, train_Loss: 1.792, val_Loss: 1.718\n",
      "Epoch: 5, train_Loss: 1.792, val_Loss: 1.720\n",
      "Epoch: 6, train_Loss: 1.792, val_Loss: 1.722\n",
      "Epoch: 7, train_Loss: 1.793, val_Loss: 1.723\n",
      "Epoch: 8, train_Loss: 1.794, val_Loss: 1.723\n",
      "Epoch: 9, train_Loss: 1.795, val_Loss: 1.722\n",
      "Epoch: 10, train_Loss: 1.795, val_Loss: 1.722\n",
      "Epoch: 11, train_Loss: 1.795, val_Loss: 1.722\n",
      "Epoch: 12, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 13, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 14, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 15, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 16, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 17, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 18, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 19, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 20, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 21, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 22, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 23, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 24, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 25, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 26, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 27, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 28, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 29, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 30, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 31, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 32, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 33, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 34, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 35, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 36, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 37, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 38, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 39, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 40, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 41, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 42, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 43, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 44, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 45, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 46, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 47, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 48, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 49, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 50, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 51, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 52, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 53, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 54, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 55, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 56, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 57, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 58, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 59, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 60, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 61, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 62, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 63, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 64, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 65, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 66, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 67, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 68, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 69, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 70, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 71, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 72, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 73, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 74, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 75, train_Loss: 1.796, val_Loss: 1.721\n",
      "Epoch: 76, train_Loss: 1.796, val_Loss: 1.721\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_Predictor(dataset)\n",
    "model = model.to(device)\n",
    "train_losses, val_losses = train(dataset, train_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a7152e-3438-43f2-9432-f1b189df9404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predect(dataset, test_dataset, model):\n",
    "    model.eval()\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=5, drop_last=True)\n",
    "\n",
    "    index = np.random.choice(len(test_dataloader))\n",
    "    \n",
    "    for batch, (x, y) in enumerate(test_dataloader):\n",
    "    \n",
    "        smiles, wavelength = x, y\n",
    "        break\n",
    "    state_h, state_c = model.init_state(5)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    y_pred, (state_h, state_c) = model(smiles.to(device), (state_h, state_c))\n",
    "    y_pred_permute = torch.permute(y_pred, (2, 1, 0))    \n",
    "\n",
    "    print(y_pred_permute)\n",
    "    print(wavelength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df41f473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.2185, -1.4125, -1.4125, -1.4125, -1.4125],\n",
      "         [-1.7355, -1.1382, -1.7493, -1.1382, -1.1382],\n",
      "         [-1.7993, -1.8008, -1.8013, -1.8008, -1.8008],\n",
      "         [-1.8088, -1.8090, -1.8090, -1.8090, -1.8090],\n",
      "         [-1.8102, -1.8102, -1.8102, -1.8102, -1.8102],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.2112],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.3000, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.3225, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.7023, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.2458, -1.8104],\n",
      "         [-1.2991, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.3560, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.6719, -1.8104],\n",
      "         [-1.7034, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.1962],\n",
      "         [-1.2691, -1.5266, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.1873, -1.8104, -1.8104, -1.8104, -1.2018],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.2450, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.1863, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104],\n",
      "         [-1.8104, -1.8104, -1.8104, -1.8104, -1.8104]]], device='cuda:0',\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "tensor([-1.2182, -2.4815, -3.0969, -3.5227, -1.5654])\n"
     ]
    }
   ],
   "source": [
    "predect(dataset, test_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc12867b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\n",
    "axes[0].plot(train_losses, label=\"train_Loss\")\n",
    "axes[0].plot(val_losses, label=\"val_Loss\")\n",
    "axes[0].grid()\n",
    "axes[0].set_xlabel(\"epoch\")\n",
    "axes[0].legend()\n",
    "\n",
    "\n",
    "axes[1].plot(train_losses, label=\"train_Loss\")\n",
    "axes[1].plot(val_losses, label=\"val_Loss\")\n",
    "axes[1].grid()\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].set_xlabel(\"epoch\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fdc21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataset, test_dataset, model):\n",
    "    model.eval()\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=len(test_dataset), drop_last=True)\n",
    "    \n",
    "    criterion = torch.nn.L1Loss()\n",
    "    \n",
    "    total_loss = 0\n",
    "    state_h, state_c = model.init_state(len(test_dataset))\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    \n",
    "    for batch, (x, y) in enumerate(test_dataloader):\n",
    "    \n",
    "        y_pred, (state_h, state_c) = model(x.to(device), (state_h, state_c))\n",
    "        y_pred_permute = torch.permute(y_pred, (2, 1, 0))    \n",
    "        loss = criterion(y_pred_permute[0, dataset.max_length-1], y.to(device))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        state_h = state_h.detach()\n",
    "        state_c = state_c.detach()\n",
    "    \n",
    "    return total_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce14c760",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 5.28 GiB (GPU 0; 23.70 GiB total capacity; 16.76 GiB already allocated; 5.11 GiB free; 17.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(test(dataset, test_dataset, model))\n",
      "Cell \u001b[0;32mIn [57], line 16\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(dataset, test_dataset, model)\u001b[0m\n\u001b[1;32m     12\u001b[0m state_c \u001b[39m=\u001b[39m state_c\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m batch, (x, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(test_dataloader):\n\u001b[0;32m---> 16\u001b[0m     y_pred, (state_h, state_c) \u001b[39m=\u001b[39m model(x\u001b[39m.\u001b[39;49mto(device), (state_h, state_c))\n\u001b[1;32m     17\u001b[0m     y_pred_permute \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mpermute(y_pred, (\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m))    \n\u001b[1;32m     18\u001b[0m     loss \u001b[39m=\u001b[39m criterion(y_pred_permute[\u001b[39m0\u001b[39m, dataset\u001b[39m.\u001b[39mmax_length\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], y\u001b[39m.\u001b[39mto(device))\n",
      "File \u001b[0;32m~/mambaforge/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [36], line 29\u001b[0m, in \u001b[0;36mLSTM_Predictor.forward\u001b[0;34m(self, x, prev_state)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, prev_state):\n\u001b[1;32m     28\u001b[0m     embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[0;32m---> 29\u001b[0m     output, state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(embed, prev_state)\n\u001b[1;32m     30\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(output)\n\u001b[1;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m logits, state\n",
      "File \u001b[0;32m~/mambaforge/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/rnn.py:769\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    770\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    771\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    773\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 5.28 GiB (GPU 0; 23.70 GiB total capacity; 16.76 GiB already allocated; 5.11 GiB free; 17.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "print(\"test_Loss: {:.3f}\".format(test(dataset, test_dataset, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cfc140",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=len(test_dataset), drop_last=True)\n",
    "\n",
    "state_h, state_c = model.init_state(len(test_dataset))\n",
    "state_h = state_h.to(device)\n",
    "state_c = state_c.to(device)\n",
    "\n",
    "for batch, (x, y) in enumerate(test_dataloader):\n",
    "\n",
    "    y_pred, (state_h, state_c) = model(x.to(device), (state_h, state_c))\n",
    "    y_pred_permute = torch.permute(y_pred, (2, 1, 0))    \n",
    "    state_h = state_h.detach()\n",
    "    state_c = state_c.detach()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(y.cpu().detach().numpy(), y_pred_permute[0, dataset.max_length-1].cpu().detach().numpy())\n",
    "plt.title(REGRESSION_COL)\n",
    "plt.xlabel(\"labels\")\n",
    "plt.ylabel(\"output\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f18a3795fe5cb6a973482da1273789e92c21cec72867792a35f0dcfcd5bea518"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
