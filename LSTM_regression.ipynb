{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f4e8431b-5b14-4ce0-8d52-8eece073a588",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 1000\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "SMILES_COL = 'Column3'\n",
    "REGRESSION_COL = 'Column62'\n",
    "URL = '/home/ishii/graduation_research/data/csvファイル/dft_B3LYP_6-31G*_zinc_for-sale_1000000_0to100000.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ca771bf3-36b2-4fec-836d-266ac0d51e60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#regression_col\n",
    "import torch\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, url, smiles_col, regression_col):\n",
    "        self.max_length = 0\n",
    "        self.dummy_char = '_'\n",
    "        \n",
    "        self.url = url\n",
    "        self.smiles_col = smiles_col\n",
    "        self.smiles = []\n",
    "        self.words = self.load_words()\n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "\n",
    "        self.regression_col = regression_col\n",
    "        self.regressions = []\n",
    "        self.items = self.generate_items()\n",
    "        \n",
    "        self.dummmy_index = self.word_to_index[self.dummy_char]\n",
    "\n",
    "    def load_words(self):\n",
    "        train_df = pd.read_csv(self.url, usecols=[SMILES_COL])\n",
    "        self.smiles = list(train_df[self.smiles_col])\n",
    "        for i, smile in enumerate(self.smiles):\n",
    "            new_smile = smile[1:]\n",
    "            self.smiles[i] = new_smile\n",
    "        self.max_length = max(len(smile) for smile in self.smiles)\n",
    "        self.smiles = list(smile.ljust(self.max_length, self.dummy_char) for smile in self.smiles)\n",
    "        train_df = pd.Series(self.smiles)\n",
    "        text = train_df.str.cat(sep=' ')\n",
    "        text = \"\".join(text.split(' '))\n",
    "        return [text[i] for i in range(len(text))]\n",
    "    \n",
    "    def generate_items(self):\n",
    "        train_df = pd.read_csv(self.url, usecols=[REGRESSION_COL])\n",
    "        self.regressions = list(train_df[self.regression_col])\n",
    "        items = []\n",
    "        for i, smile in enumerate(self.smiles):\n",
    "            smile = list(smile)\n",
    "            items.append([self.word_to_index[w] for w in smile])\n",
    "        return items\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.regressions)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.items[index]),\n",
    "            torch.tensor(self.regressions[index])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "963eec5a-0677-478b-8f00-91974d4e7ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#変更後のモデル\n",
    "import torch\n",
    "\n",
    "class LSTM_Predictor(torch.nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super(LSTM_Predictor, self).__init__()\n",
    "        self.lstm_size = 128\n",
    "        self.embedding_dim = 128\n",
    "        self.num_layers = 3\n",
    "\n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = torch.nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            #padding_idxの処理が不明確\n",
    "            #padding_idx=dataset.dummmy_index\n",
    "        )\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=self.lstm_size,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(self.lstm_size, 1)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.lstm_size),\n",
    "                torch.zeros(self.num_layers, batch_size, self.lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2cfd0211-c662-47bc-a69c-f76ddb5031c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#変更後の訓練プロセス\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def train(dataset, train_dataset, model):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, drop_last=True)\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    #scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        \n",
    "        state_h, state_c = model.init_state(BATCH_SIZE)\n",
    "        state_h = state_h.to(device)\n",
    "        state_c = state_c.to(device)\n",
    "        total_loss = 0\n",
    "        total_val_loss = 0\n",
    "\n",
    "        for batch, (x, y) in enumerate(train_dataloader):\n",
    "            if batch < int(len(train_dataloader) * 0.75):\n",
    "                model.train()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                y_pred, (state_h, state_c) = model(x.to(device), (state_h, state_c))\n",
    "                y_pred_permute = torch.permute(y_pred, (2, 1, 0))\n",
    "                loss = criterion(y_pred_permute[0][dataset.max_length-1], y.to(device))\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                state_h = state_h.detach()\n",
    "                state_c = state_c.detach()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "                y_pred, (state_h, state_c) = model(x.to(device), (state_h, state_c))\n",
    "                y_pred_permute = torch.permute(y_pred, (2, 1, 0))\n",
    "                val_loss = criterion(y_pred_permute[0, dataset.max_length-1], y.to(device))\n",
    "                total_val_loss += val_loss.item()    \n",
    "                \n",
    "                state_h = state_h.detach()\n",
    "                state_c = state_c.detach()\n",
    "        \n",
    "        #scheduler.step()\n",
    "        \n",
    "        average_total_loss = total_loss / int(len(train_dataloader) * 0.75)\n",
    "        average_total_val_loss = total_val_loss / (len(train_dataloader) - int(len(train_dataloader) * 0.75))\n",
    "        \n",
    "        print(\"Epoch: {}, train_Loss: {}, val_Loss: {}\".format(\n",
    "            epoch+1, \n",
    "            average_total_loss,\n",
    "            average_total_val_loss\n",
    "        ))\n",
    "        losses.append(average_total_loss)\n",
    "        val_losses.append(average_total_val_loss)\n",
    "    return losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a67f788d-627e-42a1-89e3-38510896c476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device: \" + str(device) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3b912c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70102\n",
      "17526\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(url=URL, smiles_col=SMILES_COL, regression_col=REGRESSION_COL)\n",
    "n_samples = len(dataset)\n",
    "indices = list(range(n_samples))\n",
    "train_size = int(n_samples * 0.8)\n",
    "test_size = n_samples - train_size\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, indices[:train_size])\n",
    "test_dataset = torch.utils.data.Subset(dataset, indices[train_size:])\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3d15e87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410\n",
      "137\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, drop_last=True)\n",
    "print(int(len(train_dataloader) * 0.75))\n",
    "print(len(train_dataloader) - int(len(train_dataloader) * 0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5b9ca541-5e56-4e0f-a42b-5c1b0ec0c701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train_Loss: 1.8425041694466662, val_Loss: 1.748183159932603\n",
      "Epoch: 2, train_Loss: 1.7922110902100075, val_Loss: 1.7260797023773193\n",
      "Epoch: 3, train_Loss: 1.7916952223312563, val_Loss: 1.7184006732745762\n",
      "Epoch: 4, train_Loss: 1.7916504896268612, val_Loss: 1.7178025019429897\n",
      "Epoch: 5, train_Loss: 1.7917918138387727, val_Loss: 1.718350705439157\n",
      "Epoch: 6, train_Loss: 1.792154639668581, val_Loss: 1.7198387775978032\n",
      "Epoch: 7, train_Loss: 1.7926581356583573, val_Loss: 1.722067413538912\n",
      "Epoch: 8, train_Loss: 1.793400623304088, val_Loss: 1.7230114201559639\n",
      "Epoch: 9, train_Loss: 1.7942836557946553, val_Loss: 1.722449253945455\n",
      "Epoch: 10, train_Loss: 1.795098170710773, val_Loss: 1.7215928291752391\n",
      "Epoch: 11, train_Loss: 1.795542931992833, val_Loss: 1.7212268296819533\n",
      "Epoch: 12, train_Loss: 1.7957218869430263, val_Loss: 1.7210253690281054\n",
      "Epoch: 13, train_Loss: 1.7958239443418456, val_Loss: 1.7209066013350105\n",
      "Epoch: 14, train_Loss: 1.7958706549028072, val_Loss: 1.720828384813601\n",
      "Epoch: 15, train_Loss: 1.7959045821573676, val_Loss: 1.7207774176214734\n",
      "Epoch: 16, train_Loss: 1.7959281882134879, val_Loss: 1.7207418175509377\n",
      "Epoch: 17, train_Loss: 1.7959441507734903, val_Loss: 1.7207164664338106\n",
      "Epoch: 18, train_Loss: 1.7959566853395323, val_Loss: 1.7206976139632455\n",
      "Epoch: 19, train_Loss: 1.7959625388064036, val_Loss: 1.7206855237919048\n",
      "Epoch: 20, train_Loss: 1.7959678513247792, val_Loss: 1.7206776076859802\n",
      "Epoch: 21, train_Loss: 1.7959712348333219, val_Loss: 1.7206716267731939\n",
      "Epoch: 22, train_Loss: 1.795973533828084, val_Loss: 1.7206680896508433\n",
      "Epoch: 23, train_Loss: 1.7959750501120963, val_Loss: 1.720665080703958\n",
      "Epoch: 24, train_Loss: 1.7959763426606248, val_Loss: 1.7206631811865924\n",
      "Epoch: 25, train_Loss: 1.795977562520562, val_Loss: 1.7206612124930334\n",
      "Epoch: 26, train_Loss: 1.795977739444593, val_Loss: 1.7206603184233618\n",
      "Epoch: 27, train_Loss: 1.7959783868091863, val_Loss: 1.7206593456059476\n",
      "Epoch: 28, train_Loss: 1.795978654303202, val_Loss: 1.7206587582609079\n",
      "Epoch: 29, train_Loss: 1.7959790453678224, val_Loss: 1.7206580560572826\n",
      "Epoch: 30, train_Loss: 1.7959792837864015, val_Loss: 1.7206578737627851\n",
      "Epoch: 31, train_Loss: 1.7959791894366102, val_Loss: 1.7206577480274394\n",
      "Epoch: 32, train_Loss: 1.7959796262950432, val_Loss: 1.7206576144608268\n",
      "Epoch: 33, train_Loss: 1.7959795643643635, val_Loss: 1.7206571328378941\n",
      "Epoch: 34, train_Loss: 1.7959795944574402, val_Loss: 1.7206569205235391\n",
      "Epoch: 35, train_Loss: 1.7959797189003084, val_Loss: 1.7206568713605839\n",
      "Epoch: 36, train_Loss: 1.7959797626588403, val_Loss: 1.7206566925466495\n",
      "Epoch: 37, train_Loss: 1.795979809470293, val_Loss: 1.7206567099494656\n",
      "Epoch: 38, train_Loss: 1.7959798179021695, val_Loss: 1.720656706033832\n",
      "Epoch: 39, train_Loss: 1.7959798840487875, val_Loss: 1.7206566925466495\n",
      "Epoch: 40, train_Loss: 1.795979815140003, val_Loss: 1.7206567773853776\n",
      "Epoch: 41, train_Loss: 1.7959798873924628, val_Loss: 1.7206567621579136\n",
      "Epoch: 42, train_Loss: 1.7959799160317678, val_Loss: 1.7206566168444\n",
      "Epoch: 43, train_Loss: 1.7959798879739715, val_Loss: 1.7206566425135537\n",
      "Epoch: 44, train_Loss: 1.7959799073091367, val_Loss: 1.7206566425135537\n",
      "Epoch: 45, train_Loss: 1.7959799560104928, val_Loss: 1.7206566072728513\n",
      "Epoch: 46, train_Loss: 1.7959799395828713, val_Loss: 1.7206566072728513\n",
      "Epoch: 47, train_Loss: 1.7959799404551344, val_Loss: 1.7206565163431375\n",
      "Epoch: 48, train_Loss: 1.7959799365299505, val_Loss: 1.7206565206938416\n",
      "Epoch: 49, train_Loss: 1.795979922719118, val_Loss: 1.7206566072728513\n",
      "Epoch: 50, train_Loss: 1.7959799307148632, val_Loss: 1.7206565650710224\n",
      "Epoch: 51, train_Loss: 1.7959799443803182, val_Loss: 1.7206565163431375\n",
      "Epoch: 52, train_Loss: 1.7959799455433356, val_Loss: 1.7206565206938416\n",
      "Epoch: 53, train_Loss: 1.7959799539752124, val_Loss: 1.7206565163431375\n",
      "Epoch: 54, train_Loss: 1.795979959354168, val_Loss: 1.7206565650710224\n",
      "Epoch: 55, train_Loss: 1.7959799587726593, val_Loss: 1.7206565163431375\n",
      "Epoch: 56, train_Loss: 1.7959799590634136, val_Loss: 1.7206565163431375\n",
      "Epoch: 57, train_Loss: 1.7959799563012473, val_Loss: 1.7206566033572177\n",
      "Epoch: 58, train_Loss: 1.795979951358423, val_Loss: 1.720656481972576\n",
      "Epoch: 59, train_Loss: 1.7959799615348258, val_Loss: 1.7206566033572177\n",
      "Epoch: 60, train_Loss: 1.7959799499046512, val_Loss: 1.7206566033572177\n",
      "Epoch: 61, train_Loss: 1.7959799564466243, val_Loss: 1.720656481972576\n",
      "Epoch: 62, train_Loss: 1.7959799536844578, val_Loss: 1.7206566033572177\n",
      "Epoch: 63, train_Loss: 1.7959799581911506, val_Loss: 1.720656481972576\n",
      "Epoch: 64, train_Loss: 1.79597996182558, val_Loss: 1.7206565163431375\n",
      "Epoch: 65, train_Loss: 1.7959799564466243, val_Loss: 1.720656481972576\n",
      "Epoch: 66, train_Loss: 1.7959799571735102, val_Loss: 1.7206566033572177\n",
      "Epoch: 67, train_Loss: 1.7959799606625626, val_Loss: 1.7206565163431375\n",
      "Epoch: 68, train_Loss: 1.7959799592087908, val_Loss: 1.7206565650710224\n",
      "Epoch: 69, train_Loss: 1.7959799574642648, val_Loss: 1.7206566033572177\n",
      "Epoch: 70, train_Loss: 1.7959799672045358, val_Loss: 1.720656523304264\n",
      "Epoch: 71, train_Loss: 1.7959799693851937, val_Loss: 1.720656523304264\n",
      "Epoch: 72, train_Loss: 1.7959799666230272, val_Loss: 1.7206566033572177\n",
      "Epoch: 73, train_Loss: 1.7959799635701064, val_Loss: 1.7206566033572177\n",
      "Epoch: 74, train_Loss: 1.7959799701120795, val_Loss: 1.720656523304264\n",
      "Epoch: 75, train_Loss: 1.7959799554289841, val_Loss: 1.7206565163431375\n",
      "Epoch: 76, train_Loss: 1.79597996182558, val_Loss: 1.720656523304264\n",
      "Epoch: 77, train_Loss: 1.7959799596449224, val_Loss: 1.720656481972576\n",
      "Epoch: 78, train_Loss: 1.7959799594995451, val_Loss: 1.720656523304264\n",
      "Epoch: 79, train_Loss: 1.7959799547020983, val_Loss: 1.7206565163431375\n",
      "Epoch: 80, train_Loss: 1.795979964006238, val_Loss: 1.720656481972576\n",
      "Epoch: 81, train_Loss: 1.7959799644423695, val_Loss: 1.720656481972576\n",
      "Epoch: 82, train_Loss: 1.7959799613894485, val_Loss: 1.720656481972576\n",
      "Epoch: 83, train_Loss: 1.795979960226431, val_Loss: 1.7206566033572177\n",
      "Epoch: 84, train_Loss: 1.7959799603718083, val_Loss: 1.7206566033572177\n",
      "Epoch: 85, train_Loss: 1.795979964006238, val_Loss: 1.7206566033572177\n",
      "Epoch: 86, train_Loss: 1.7959799610986942, val_Loss: 1.7206566033572177\n",
      "Epoch: 87, train_Loss: 1.7959799612440714, val_Loss: 1.720656481972576\n",
      "Epoch: 88, train_Loss: 1.7959799692398164, val_Loss: 1.720656481972576\n",
      "Epoch: 89, train_Loss: 1.795979952957572, val_Loss: 1.720656481972576\n",
      "Epoch: 90, train_Loss: 1.7959799555743612, val_Loss: 1.720656481972576\n",
      "Epoch: 91, train_Loss: 1.7959799535390808, val_Loss: 1.7206566033572177\n",
      "Epoch: 92, train_Loss: 1.7959799610986942, val_Loss: 1.720656481972576\n",
      "Epoch: 93, train_Loss: 1.7959799605171856, val_Loss: 1.720656481972576\n",
      "Epoch: 94, train_Loss: 1.7959799478693705, val_Loss: 1.720656481972576\n",
      "Epoch: 95, train_Loss: 1.7959799606625626, val_Loss: 1.720656481972576\n",
      "Epoch: 96, train_Loss: 1.7959799567373786, val_Loss: 1.720656481972576\n",
      "Epoch: 97, train_Loss: 1.795979957755019, val_Loss: 1.7206566033572177\n",
      "Epoch: 98, train_Loss: 1.7959799610986942, val_Loss: 1.7206566033572177\n",
      "Epoch: 99, train_Loss: 1.7959799558651157, val_Loss: 1.7206566033572177\n",
      "Epoch: 100, train_Loss: 1.7959799563012473, val_Loss: 1.720656481972576\n",
      "Epoch: 101, train_Loss: 1.795979957755019, val_Loss: 1.7206566033572177\n",
      "Epoch: 102, train_Loss: 1.7959799558651157, val_Loss: 1.7206566033572177\n",
      "Epoch: 103, train_Loss: 1.7959799587726593, val_Loss: 1.7206566033572177\n",
      "Epoch: 104, train_Loss: 1.795979952812195, val_Loss: 1.720656523304264\n",
      "Epoch: 105, train_Loss: 1.7959799619709573, val_Loss: 1.720656481972576\n",
      "Epoch: 106, train_Loss: 1.7959799552836069, val_Loss: 1.7206566033572177\n",
      "Epoch: 107, train_Loss: 1.795979956882756, val_Loss: 1.7206566033572177\n",
      "Epoch: 108, train_Loss: 1.7959799563012473, val_Loss: 1.720656481972576\n",
      "Epoch: 109, train_Loss: 1.7959799609533171, val_Loss: 1.720656481972576\n",
      "Epoch: 110, train_Loss: 1.795979965750764, val_Loss: 1.7206566033572177\n",
      "Epoch: 111, train_Loss: 1.7959799609533171, val_Loss: 1.720656481972576\n",
      "Epoch: 112, train_Loss: 1.7959799576096418, val_Loss: 1.7206565163431375\n",
      "Epoch: 113, train_Loss: 1.79597996182558, val_Loss: 1.720656523304264\n",
      "Epoch: 114, train_Loss: 1.7959799609533171, val_Loss: 1.7206565163431375\n",
      "Epoch: 115, train_Loss: 1.7959799631339748, val_Loss: 1.7206565163431375\n",
      "Epoch: 116, train_Loss: 1.7959799525214404, val_Loss: 1.7206565163431375\n",
      "Epoch: 117, train_Loss: 1.7959799551382298, val_Loss: 1.7206564919791953\n",
      "Epoch: 118, train_Loss: 1.7959799586272822, val_Loss: 1.7206565163431375\n",
      "Epoch: 119, train_Loss: 1.795979961680203, val_Loss: 1.7206566033572177\n",
      "Epoch: 120, train_Loss: 1.7959799619709573, val_Loss: 1.7206565163431375\n",
      "Epoch: 121, train_Loss: 1.7959799587726593, val_Loss: 1.720656481972576\n",
      "Epoch: 122, train_Loss: 1.795979957900396, val_Loss: 1.720656481972576\n",
      "Epoch: 123, train_Loss: 1.795979960226431, val_Loss: 1.7206565163431375\n",
      "Epoch: 124, train_Loss: 1.795979961680203, val_Loss: 1.720656481972576\n",
      "Epoch: 125, train_Loss: 1.795979959354168, val_Loss: 1.720656481972576\n",
      "Epoch: 126, train_Loss: 1.795979959354168, val_Loss: 1.720656481972576\n",
      "Epoch: 127, train_Loss: 1.7959799533937035, val_Loss: 1.7206565163431375\n",
      "Epoch: 128, train_Loss: 1.7959799567373786, val_Loss: 1.720656481972576\n",
      "Epoch: 129, train_Loss: 1.7959799621163346, val_Loss: 1.7206566033572177\n",
      "Epoch: 130, train_Loss: 1.7959799587726593, val_Loss: 1.7206566033572177\n",
      "Epoch: 131, train_Loss: 1.7959799610986942, val_Loss: 1.7206566033572177\n",
      "Epoch: 132, train_Loss: 1.795979948887011, val_Loss: 1.720656481972576\n",
      "Epoch: 133, train_Loss: 1.7959799551382298, val_Loss: 1.7206566033572177\n",
      "Epoch: 134, train_Loss: 1.7959799644423695, val_Loss: 1.720656481972576\n",
      "Epoch: 135, train_Loss: 1.7959799610986942, val_Loss: 1.720656523304264\n",
      "Epoch: 136, train_Loss: 1.795979960226431, val_Loss: 1.720656481972576\n",
      "Epoch: 137, train_Loss: 1.7959799597902997, val_Loss: 1.720656481972576\n",
      "Epoch: 138, train_Loss: 1.7959799544113437, val_Loss: 1.7206565163431375\n",
      "Epoch: 139, train_Loss: 1.795979960226431, val_Loss: 1.7206566033572177\n",
      "Epoch: 140, train_Loss: 1.7959799613894485, val_Loss: 1.7206565650710224\n",
      "Epoch: 141, train_Loss: 1.7959799647331238, val_Loss: 1.720656481972576\n",
      "Epoch: 142, train_Loss: 1.795979952957572, val_Loss: 1.7206566033572177\n",
      "Epoch: 143, train_Loss: 1.7959799496138968, val_Loss: 1.7206565163431375\n",
      "Epoch: 144, train_Loss: 1.7959799615348258, val_Loss: 1.7206565650710224\n",
      "Epoch: 145, train_Loss: 1.7959799557197385, val_Loss: 1.7206566033572177\n",
      "Epoch: 146, train_Loss: 1.7959799613894485, val_Loss: 1.7206566033572177\n",
      "Epoch: 147, train_Loss: 1.7959799608079399, val_Loss: 1.720656481972576\n",
      "Epoch: 148, train_Loss: 1.795979963279352, val_Loss: 1.720656481972576\n",
      "Epoch: 149, train_Loss: 1.7959799491777653, val_Loss: 1.7206565163431375\n",
      "Epoch: 150, train_Loss: 1.7959799606625626, val_Loss: 1.7206565650710224\n",
      "Epoch: 151, train_Loss: 1.7959799637154834, val_Loss: 1.7206565650710224\n",
      "Epoch: 152, train_Loss: 1.795979963424729, val_Loss: 1.720656481972576\n",
      "Epoch: 153, train_Loss: 1.7959799547020983, val_Loss: 1.720656481972576\n",
      "Epoch: 154, train_Loss: 1.795979952230686, val_Loss: 1.7206566033572177\n",
      "Epoch: 155, train_Loss: 1.7959799619709573, val_Loss: 1.7206566033572177\n",
      "Epoch: 156, train_Loss: 1.795979963279352, val_Loss: 1.7206566033572177\n",
      "Epoch: 157, train_Loss: 1.7959799621163346, val_Loss: 1.7206566033572177\n",
      "Epoch: 158, train_Loss: 1.7959799612440714, val_Loss: 1.720656481972576\n",
      "Epoch: 159, train_Loss: 1.7959799555743612, val_Loss: 1.7206565163431375\n",
      "Epoch: 160, train_Loss: 1.7959799499046512, val_Loss: 1.7206566033572177\n",
      "Epoch: 161, train_Loss: 1.7959799567373786, val_Loss: 1.7206566033572177\n",
      "Epoch: 162, train_Loss: 1.7959799576096418, val_Loss: 1.720656523304264\n",
      "Epoch: 163, train_Loss: 1.7959799573188875, val_Loss: 1.720656523304264\n",
      "Epoch: 164, train_Loss: 1.7959799645877466, val_Loss: 1.7206566033572177\n",
      "Epoch: 165, train_Loss: 1.795979967349913, val_Loss: 1.7206565163431375\n",
      "Epoch: 166, train_Loss: 1.795979968949062, val_Loss: 1.7206565650710224\n",
      "Epoch: 167, train_Loss: 1.7959799615348258, val_Loss: 1.7206565163431375\n",
      "Epoch: 168, train_Loss: 1.7959799626978432, val_Loss: 1.7206565163431375\n",
      "Epoch: 169, train_Loss: 1.7959799603718083, val_Loss: 1.7206566033572177\n",
      "Epoch: 170, train_Loss: 1.7959799501954055, val_Loss: 1.7206566033572177\n",
      "Epoch: 171, train_Loss: 1.7959799587726593, val_Loss: 1.720656481972576\n",
      "Epoch: 172, train_Loss: 1.7959799565920016, val_Loss: 1.720656523304264\n",
      "Epoch: 173, train_Loss: 1.7959799612440714, val_Loss: 1.7206566033572177\n",
      "Epoch: 174, train_Loss: 1.7959799677860446, val_Loss: 1.7206565650710224\n",
      "Epoch: 175, train_Loss: 1.7959799576096418, val_Loss: 1.7206565163431375\n",
      "Epoch: 176, train_Loss: 1.7959799609533171, val_Loss: 1.7206565163431375\n",
      "Epoch: 177, train_Loss: 1.7959799563012473, val_Loss: 1.720656481972576\n",
      "Epoch: 178, train_Loss: 1.7959799586272822, val_Loss: 1.7206566033572177\n",
      "Epoch: 179, train_Loss: 1.7959799570281332, val_Loss: 1.7206566033572177\n",
      "Epoch: 180, train_Loss: 1.795979961680203, val_Loss: 1.7206566033572177\n",
      "Epoch: 181, train_Loss: 1.7959799608079399, val_Loss: 1.7206566033572177\n",
      "Epoch: 182, train_Loss: 1.7959799581911506, val_Loss: 1.7206565163431375\n",
      "Epoch: 183, train_Loss: 1.7959799613894485, val_Loss: 1.7206566033572177\n",
      "Epoch: 184, train_Loss: 1.7959799619709573, val_Loss: 1.7206566033572177\n",
      "Epoch: 185, train_Loss: 1.7959799548474753, val_Loss: 1.720656481972576\n",
      "Epoch: 186, train_Loss: 1.7959799587726593, val_Loss: 1.7206566033572177\n",
      "Epoch: 187, train_Loss: 1.7959799552836069, val_Loss: 1.720656523304264\n",
      "Epoch: 188, train_Loss: 1.7959799590634136, val_Loss: 1.720656523304264\n",
      "Epoch: 189, train_Loss: 1.7959799638608607, val_Loss: 1.7206565163431375\n",
      "Epoch: 190, train_Loss: 1.79597996182558, val_Loss: 1.7206565163431375\n",
      "Epoch: 191, train_Loss: 1.7959799583365277, val_Loss: 1.720656481972576\n",
      "Epoch: 192, train_Loss: 1.7959799629885975, val_Loss: 1.720656481972576\n",
      "Epoch: 193, train_Loss: 1.7959799554289841, val_Loss: 1.7206565163431375\n",
      "Epoch: 194, train_Loss: 1.7959799606625626, val_Loss: 1.720656481972576\n",
      "Epoch: 195, train_Loss: 1.7959799589180365, val_Loss: 1.7206566033572177\n",
      "Epoch: 196, train_Loss: 1.7959799615348258, val_Loss: 1.7206566033572177\n",
      "Epoch: 197, train_Loss: 1.795979963424729, val_Loss: 1.7206566033572177\n",
      "Epoch: 198, train_Loss: 1.7959799610986942, val_Loss: 1.720656481972576\n",
      "Epoch: 199, train_Loss: 1.795979960081054, val_Loss: 1.720656481972576\n",
      "Epoch: 200, train_Loss: 1.7959799626978432, val_Loss: 1.720656481972576\n",
      "Epoch: 201, train_Loss: 1.7959799596449224, val_Loss: 1.720656481972576\n",
      "Epoch: 202, train_Loss: 1.7959799557197385, val_Loss: 1.720656481972576\n",
      "Epoch: 203, train_Loss: 1.795979964151615, val_Loss: 1.7206566033572177\n",
      "Epoch: 204, train_Loss: 1.7959799581911506, val_Loss: 1.7206566033572177\n",
      "Epoch: 205, train_Loss: 1.7959799658961413, val_Loss: 1.720656523304264\n",
      "Epoch: 206, train_Loss: 1.7959799532483263, val_Loss: 1.720656523304264\n",
      "Epoch: 207, train_Loss: 1.795979965750764, val_Loss: 1.7206566033572177\n",
      "Epoch: 208, train_Loss: 1.7959799613894485, val_Loss: 1.720656481972576\n",
      "Epoch: 209, train_Loss: 1.7959799541205894, val_Loss: 1.7206566033572177\n",
      "Epoch: 210, train_Loss: 1.795979949032388, val_Loss: 1.7206566033572177\n",
      "Epoch: 211, train_Loss: 1.795979967349913, val_Loss: 1.7206565163431375\n",
      "Epoch: 212, train_Loss: 1.7959799580457734, val_Loss: 1.7206566033572177\n",
      "Epoch: 213, train_Loss: 1.795979957755019, val_Loss: 1.720656523304264\n",
      "Epoch: 214, train_Loss: 1.7959799605171856, val_Loss: 1.7206566033572177\n",
      "Epoch: 215, train_Loss: 1.7959799603718083, val_Loss: 1.7206566033572177\n",
      "Epoch: 216, train_Loss: 1.7959799599356767, val_Loss: 1.720656481972576\n",
      "Epoch: 217, train_Loss: 1.7959799619709573, val_Loss: 1.7206566033572177\n",
      "Epoch: 218, train_Loss: 1.7959799647331238, val_Loss: 1.7206566033572177\n",
      "Epoch: 219, train_Loss: 1.7959799622617116, val_Loss: 1.720656523304264\n",
      "Epoch: 220, train_Loss: 1.7959799648785009, val_Loss: 1.7206565163431375\n",
      "Epoch: 221, train_Loss: 1.795979957900396, val_Loss: 1.7206565163431375\n",
      "Epoch: 222, train_Loss: 1.7959799610986942, val_Loss: 1.7206566033572177\n",
      "Epoch: 223, train_Loss: 1.795979965605387, val_Loss: 1.7206566033572177\n",
      "Epoch: 224, train_Loss: 1.795979963279352, val_Loss: 1.720656523304264\n",
      "Epoch: 225, train_Loss: 1.7959799637154834, val_Loss: 1.720656523304264\n",
      "Epoch: 226, train_Loss: 1.7959799615348258, val_Loss: 1.7206566033572177\n",
      "Epoch: 227, train_Loss: 1.7959799597902997, val_Loss: 1.7206566033572177\n",
      "Epoch: 228, train_Loss: 1.7959799610986942, val_Loss: 1.720656481972576\n",
      "Epoch: 229, train_Loss: 1.7959799621163346, val_Loss: 1.720656523304264\n",
      "Epoch: 230, train_Loss: 1.7959799619709573, val_Loss: 1.7206566033572177\n",
      "Epoch: 231, train_Loss: 1.7959799605171856, val_Loss: 1.720656481972576\n",
      "Epoch: 232, train_Loss: 1.79597996647765, val_Loss: 1.720656481972576\n",
      "Epoch: 233, train_Loss: 1.795979960081054, val_Loss: 1.720656481972576\n",
      "Epoch: 234, train_Loss: 1.7959799558651157, val_Loss: 1.720656523304264\n",
      "Epoch: 235, train_Loss: 1.7959799610986942, val_Loss: 1.720656523304264\n",
      "Epoch: 236, train_Loss: 1.7959799509222913, val_Loss: 1.7206565163431375\n",
      "Epoch: 237, train_Loss: 1.7959799613894485, val_Loss: 1.7206566033572177\n",
      "Epoch: 238, train_Loss: 1.7959799594995451, val_Loss: 1.7206566033572177\n",
      "Epoch: 239, train_Loss: 1.795979959354168, val_Loss: 1.7206564919791953\n",
      "Epoch: 240, train_Loss: 1.7959799676406674, val_Loss: 1.7206566033572177\n",
      "Epoch: 241, train_Loss: 1.7959799590634136, val_Loss: 1.720656481972576\n",
      "Epoch: 242, train_Loss: 1.7959799536844578, val_Loss: 1.720656481972576\n",
      "Epoch: 243, train_Loss: 1.7959799654600097, val_Loss: 1.720656523304264\n",
      "Epoch: 244, train_Loss: 1.7959799688036848, val_Loss: 1.7206566033572177\n",
      "Epoch: 245, train_Loss: 1.7959799721473602, val_Loss: 1.7206565163431375\n",
      "Epoch: 246, train_Loss: 1.7959799555743612, val_Loss: 1.7206566033572177\n",
      "Epoch: 247, train_Loss: 1.795979962552466, val_Loss: 1.7206566033572177\n",
      "Epoch: 248, train_Loss: 1.7959799597902997, val_Loss: 1.7206566033572177\n",
      "Epoch: 249, train_Loss: 1.7959799583365277, val_Loss: 1.7206566033572177\n",
      "Epoch: 250, train_Loss: 1.7959799650238781, val_Loss: 1.720656481972576\n",
      "Epoch: 251, train_Loss: 1.795979958481905, val_Loss: 1.7206566033572177\n",
      "Epoch: 252, train_Loss: 1.7959799590634136, val_Loss: 1.720656481972576\n",
      "Epoch: 253, train_Loss: 1.795979962407089, val_Loss: 1.7206566033572177\n",
      "Epoch: 254, train_Loss: 1.7959799592087908, val_Loss: 1.7206566033572177\n",
      "Epoch: 255, train_Loss: 1.795979958481905, val_Loss: 1.720656523304264\n",
      "Epoch: 256, train_Loss: 1.795979964151615, val_Loss: 1.720656481972576\n",
      "Epoch: 257, train_Loss: 1.7959799619709573, val_Loss: 1.7206565163431375\n",
      "Epoch: 258, train_Loss: 1.7959799599356767, val_Loss: 1.7206565163431375\n",
      "Epoch: 259, train_Loss: 1.7959799610986942, val_Loss: 1.720656481972576\n",
      "Epoch: 260, train_Loss: 1.7959799551382298, val_Loss: 1.7206565163431375\n",
      "Epoch: 261, train_Loss: 1.795979961680203, val_Loss: 1.720656523304264\n",
      "Epoch: 262, train_Loss: 1.795979952957572, val_Loss: 1.720656481972576\n",
      "Epoch: 263, train_Loss: 1.7959799563012473, val_Loss: 1.7206565163431375\n",
      "Epoch: 264, train_Loss: 1.79597995615587, val_Loss: 1.7206566033572177\n",
      "Epoch: 265, train_Loss: 1.7959799635701064, val_Loss: 1.7206566033572177\n",
      "Epoch: 266, train_Loss: 1.795979958481905, val_Loss: 1.7206566033572177\n",
      "Epoch: 267, train_Loss: 1.7959799558651157, val_Loss: 1.720656523304264\n",
      "Epoch: 268, train_Loss: 1.7959799621163346, val_Loss: 1.720656523304264\n",
      "Epoch: 269, train_Loss: 1.7959799638608607, val_Loss: 1.7206566033572177\n",
      "Epoch: 270, train_Loss: 1.7959799693851937, val_Loss: 1.720656481972576\n",
      "Epoch: 271, train_Loss: 1.795979958481905, val_Loss: 1.720656481972576\n",
      "Epoch: 272, train_Loss: 1.795979964151615, val_Loss: 1.7206566033572177\n",
      "Epoch: 273, train_Loss: 1.795979957900396, val_Loss: 1.720656523304264\n",
      "Epoch: 274, train_Loss: 1.7959799631339748, val_Loss: 1.720656481972576\n",
      "Epoch: 275, train_Loss: 1.7959799589180365, val_Loss: 1.720656481972576\n",
      "Epoch: 276, train_Loss: 1.795979962552466, val_Loss: 1.7206565163431375\n",
      "Epoch: 277, train_Loss: 1.7959799567373786, val_Loss: 1.7206565163431375\n",
      "Epoch: 278, train_Loss: 1.7959799520853088, val_Loss: 1.7206565163431375\n",
      "Epoch: 279, train_Loss: 1.795979961680203, val_Loss: 1.7206565163431375\n",
      "Epoch: 280, train_Loss: 1.795979957900396, val_Loss: 1.7206566033572177\n",
      "Epoch: 281, train_Loss: 1.7959799520853088, val_Loss: 1.7206566033572177\n",
      "Epoch: 282, train_Loss: 1.7959799622617116, val_Loss: 1.7206566033572177\n",
      "Epoch: 283, train_Loss: 1.7959799653146324, val_Loss: 1.720656481972576\n",
      "Epoch: 284, train_Loss: 1.7959799567373786, val_Loss: 1.720656481972576\n",
      "Epoch: 285, train_Loss: 1.7959799635701064, val_Loss: 1.7206566033572177\n",
      "Epoch: 286, train_Loss: 1.795979959354168, val_Loss: 1.7206565163431375\n",
      "Epoch: 287, train_Loss: 1.7959799583365277, val_Loss: 1.720656481972576\n",
      "Epoch: 288, train_Loss: 1.7959799583365277, val_Loss: 1.7206566033572177\n",
      "Epoch: 289, train_Loss: 1.7959799468517303, val_Loss: 1.7206566033572177\n",
      "Epoch: 290, train_Loss: 1.7959799531029492, val_Loss: 1.7206566033572177\n",
      "Epoch: 291, train_Loss: 1.7959799551382298, val_Loss: 1.720656481972576\n",
      "Epoch: 292, train_Loss: 1.7959799532483263, val_Loss: 1.720656523304264\n",
      "Epoch: 293, train_Loss: 1.7959799612440714, val_Loss: 1.720656481972576\n",
      "Epoch: 294, train_Loss: 1.7959799555743612, val_Loss: 1.7206566033572177\n",
      "Epoch: 295, train_Loss: 1.795979960226431, val_Loss: 1.7206566033572177\n",
      "Epoch: 296, train_Loss: 1.795979960081054, val_Loss: 1.7206565163431375\n",
      "Epoch: 297, train_Loss: 1.7959799705482111, val_Loss: 1.720656523304264\n",
      "Epoch: 298, train_Loss: 1.7959799564466243, val_Loss: 1.7206566033572177\n",
      "Epoch: 299, train_Loss: 1.795979958481905, val_Loss: 1.7206565163431375\n",
      "Epoch: 300, train_Loss: 1.7959799590634136, val_Loss: 1.7206565163431375\n",
      "Epoch: 301, train_Loss: 1.795979957755019, val_Loss: 1.720656481972576\n",
      "Epoch: 302, train_Loss: 1.795979960081054, val_Loss: 1.7206566033572177\n",
      "Epoch: 303, train_Loss: 1.7959799525214404, val_Loss: 1.7206566033572177\n",
      "Epoch: 304, train_Loss: 1.7959799589180365, val_Loss: 1.7206566033572177\n",
      "Epoch: 305, train_Loss: 1.7959799592087908, val_Loss: 1.7206566033572177\n",
      "Epoch: 306, train_Loss: 1.7959799629885975, val_Loss: 1.7206566033572177\n",
      "Epoch: 307, train_Loss: 1.7959799555743612, val_Loss: 1.7206566033572177\n",
      "Epoch: 308, train_Loss: 1.7959799661868956, val_Loss: 1.7206566033572177\n",
      "Epoch: 309, train_Loss: 1.7959799670591587, val_Loss: 1.7206566033572177\n",
      "Epoch: 310, train_Loss: 1.7959799573188875, val_Loss: 1.720656481972576\n",
      "Epoch: 311, train_Loss: 1.795979963279352, val_Loss: 1.720656481972576\n",
      "Epoch: 312, train_Loss: 1.795979965750764, val_Loss: 1.720656481972576\n",
      "Epoch: 313, train_Loss: 1.7959799547020983, val_Loss: 1.7206565163431375\n",
      "Epoch: 314, train_Loss: 1.795979960226431, val_Loss: 1.7206566033572177\n",
      "Epoch: 315, train_Loss: 1.7959799608079399, val_Loss: 1.7206566033572177\n",
      "Epoch: 316, train_Loss: 1.7959799564466243, val_Loss: 1.7206566033572177\n",
      "Epoch: 317, train_Loss: 1.7959799587726593, val_Loss: 1.7206566033572177\n",
      "Epoch: 318, train_Loss: 1.7959799609533171, val_Loss: 1.7206565163431375\n",
      "Epoch: 319, train_Loss: 1.7959799567373786, val_Loss: 1.7206566033572177\n",
      "Epoch: 320, train_Loss: 1.7959799517945545, val_Loss: 1.720656481972576\n",
      "Epoch: 321, train_Loss: 1.7959799512130459, val_Loss: 1.720656481972576\n",
      "Epoch: 322, train_Loss: 1.7959799519399318, val_Loss: 1.720656523304264\n",
      "Epoch: 323, train_Loss: 1.7959799613894485, val_Loss: 1.7206566033572177\n",
      "Epoch: 324, train_Loss: 1.795979963424729, val_Loss: 1.7206565650710224\n",
      "Epoch: 325, train_Loss: 1.7959799613894485, val_Loss: 1.720656481972576\n",
      "Epoch: 326, train_Loss: 1.79597996182558, val_Loss: 1.720656481972576\n",
      "Epoch: 327, train_Loss: 1.7959799610986942, val_Loss: 1.720656481972576\n",
      "Epoch: 328, train_Loss: 1.7959799552836069, val_Loss: 1.7206566033572177\n",
      "Epoch: 329, train_Loss: 1.7959799648785009, val_Loss: 1.7206566033572177\n",
      "Epoch: 330, train_Loss: 1.7959799580457734, val_Loss: 1.720656481972576\n",
      "Epoch: 331, train_Loss: 1.7959799605171856, val_Loss: 1.720656481972576\n",
      "Epoch: 332, train_Loss: 1.7959799590634136, val_Loss: 1.720656481972576\n",
      "Epoch: 333, train_Loss: 1.795979965750764, val_Loss: 1.720656523304264\n",
      "Epoch: 334, train_Loss: 1.7959799628432205, val_Loss: 1.7206566033572177\n",
      "Epoch: 335, train_Loss: 1.7959799603718083, val_Loss: 1.7206566033572177\n",
      "Epoch: 336, train_Loss: 1.7959799552836069, val_Loss: 1.720656523304264\n",
      "Epoch: 337, train_Loss: 1.795979963279352, val_Loss: 1.7206564919791953\n",
      "Epoch: 338, train_Loss: 1.79597995615587, val_Loss: 1.720656481972576\n",
      "Epoch: 339, train_Loss: 1.7959799610986942, val_Loss: 1.720656481972576\n",
      "Epoch: 340, train_Loss: 1.795979952230686, val_Loss: 1.7206566033572177\n",
      "Epoch: 341, train_Loss: 1.7959799612440714, val_Loss: 1.7206566033572177\n",
      "Epoch: 342, train_Loss: 1.7959799516491775, val_Loss: 1.7206566033572177\n",
      "Epoch: 343, train_Loss: 1.7959799497592739, val_Loss: 1.720656481972576\n",
      "Epoch: 344, train_Loss: 1.7959799629885975, val_Loss: 1.720656481972576\n",
      "Epoch: 345, train_Loss: 1.7959799596449224, val_Loss: 1.720656481972576\n",
      "Epoch: 346, train_Loss: 1.795979963424729, val_Loss: 1.7206565163431375\n",
      "Epoch: 347, train_Loss: 1.795979957755019, val_Loss: 1.720656481972576\n",
      "Epoch: 348, train_Loss: 1.7959799574642648, val_Loss: 1.7206566033572177\n",
      "Epoch: 349, train_Loss: 1.795979963424729, val_Loss: 1.720656481972576\n",
      "Epoch: 350, train_Loss: 1.7959799480147478, val_Loss: 1.7206566033572177\n",
      "Epoch: 351, train_Loss: 1.7959799589180365, val_Loss: 1.7206566033572177\n",
      "Epoch: 352, train_Loss: 1.795979964006238, val_Loss: 1.7206566033572177\n",
      "Epoch: 353, train_Loss: 1.7959799551382298, val_Loss: 1.7206566033572177\n",
      "Epoch: 354, train_Loss: 1.7959799583365277, val_Loss: 1.7206566033572177\n",
      "Epoch: 355, train_Loss: 1.7959799594995451, val_Loss: 1.7206566033572177\n",
      "Epoch: 356, train_Loss: 1.7959799590634136, val_Loss: 1.720656481972576\n",
      "Epoch: 357, train_Loss: 1.7959799554289841, val_Loss: 1.7206566033572177\n",
      "Epoch: 358, train_Loss: 1.7959799695305707, val_Loss: 1.720656481972576\n",
      "Epoch: 359, train_Loss: 1.7959799558651157, val_Loss: 1.720656481972576\n",
      "Epoch: 360, train_Loss: 1.7959799644423695, val_Loss: 1.720656481972576\n",
      "Epoch: 361, train_Loss: 1.7959799571735102, val_Loss: 1.720656481972576\n",
      "Epoch: 362, train_Loss: 1.795979952957572, val_Loss: 1.7206566033572177\n",
      "Epoch: 363, train_Loss: 1.7959799564466243, val_Loss: 1.7206566033572177\n",
      "Epoch: 364, train_Loss: 1.7959799573188875, val_Loss: 1.7206566033572177\n",
      "Epoch: 365, train_Loss: 1.7959799544113437, val_Loss: 1.7206564919791953\n",
      "Epoch: 366, train_Loss: 1.7959799606625626, val_Loss: 1.7206565650710224\n",
      "Epoch: 367, train_Loss: 1.7959799644423695, val_Loss: 1.720656481972576\n",
      "Epoch: 368, train_Loss: 1.7959799596449224, val_Loss: 1.720656481972576\n",
      "Epoch: 369, train_Loss: 1.7959799493231425, val_Loss: 1.7206566033572177\n",
      "Epoch: 370, train_Loss: 1.7959799571735102, val_Loss: 1.7206566033572177\n",
      "Epoch: 371, train_Loss: 1.79597996182558, val_Loss: 1.720656523304264\n",
      "Epoch: 372, train_Loss: 1.7959799573188875, val_Loss: 1.7206566033572177\n",
      "Epoch: 373, train_Loss: 1.7959799647331238, val_Loss: 1.7206566033572177\n",
      "Epoch: 374, train_Loss: 1.7959799590634136, val_Loss: 1.7206565650710224\n",
      "Epoch: 375, train_Loss: 1.7959799619709573, val_Loss: 1.720656481972576\n",
      "Epoch: 376, train_Loss: 1.7959799541205894, val_Loss: 1.720656481972576\n",
      "Epoch: 377, train_Loss: 1.7959799494685196, val_Loss: 1.720656481972576\n",
      "Epoch: 378, train_Loss: 1.7959799613894485, val_Loss: 1.720656481972576\n",
      "Epoch: 379, train_Loss: 1.795979957755019, val_Loss: 1.7206566033572177\n",
      "Epoch: 380, train_Loss: 1.7959799653146324, val_Loss: 1.7206566033572177\n",
      "Epoch: 381, train_Loss: 1.795979960081054, val_Loss: 1.720656481972576\n",
      "Epoch: 382, train_Loss: 1.7959799589180365, val_Loss: 1.7206566033572177\n",
      "Epoch: 383, train_Loss: 1.7959799554289841, val_Loss: 1.7206566033572177\n",
      "Epoch: 384, train_Loss: 1.7959799555743612, val_Loss: 1.7206566033572177\n",
      "Epoch: 385, train_Loss: 1.79597996182558, val_Loss: 1.720656481972576\n",
      "Epoch: 386, train_Loss: 1.795979960226431, val_Loss: 1.720656481972576\n",
      "Epoch: 387, train_Loss: 1.7959799500500284, val_Loss: 1.7206566033572177\n",
      "Epoch: 388, train_Loss: 1.7959799580457734, val_Loss: 1.7206566033572177\n",
      "Epoch: 389, train_Loss: 1.7959799551382298, val_Loss: 1.720656523304264\n",
      "Epoch: 390, train_Loss: 1.7959799615348258, val_Loss: 1.720656481972576\n",
      "Epoch: 391, train_Loss: 1.795979963279352, val_Loss: 1.7206565163431375\n",
      "Epoch: 392, train_Loss: 1.795979959354168, val_Loss: 1.7206566033572177\n",
      "Epoch: 393, train_Loss: 1.7959799606625626, val_Loss: 1.7206566033572177\n",
      "Epoch: 394, train_Loss: 1.7959799592087908, val_Loss: 1.720656481972576\n",
      "Epoch: 395, train_Loss: 1.7959799586272822, val_Loss: 1.7206566033572177\n",
      "Epoch: 396, train_Loss: 1.7959799586272822, val_Loss: 1.7206566033572177\n",
      "Epoch: 397, train_Loss: 1.7959799608079399, val_Loss: 1.720656481972576\n",
      "Epoch: 398, train_Loss: 1.7959799586272822, val_Loss: 1.720656523304264\n",
      "Epoch: 399, train_Loss: 1.7959799648785009, val_Loss: 1.7206566033572177\n",
      "Epoch: 400, train_Loss: 1.7959799552836069, val_Loss: 1.720656481972576\n",
      "Epoch: 401, train_Loss: 1.7959799576096418, val_Loss: 1.720656481972576\n",
      "Epoch: 402, train_Loss: 1.79597996647765, val_Loss: 1.720656523304264\n",
      "Epoch: 403, train_Loss: 1.7959799594995451, val_Loss: 1.720656523304264\n",
      "Epoch: 404, train_Loss: 1.7959799581911506, val_Loss: 1.7206566033572177\n",
      "Epoch: 405, train_Loss: 1.795979968076799, val_Loss: 1.7206566033572177\n",
      "Epoch: 406, train_Loss: 1.7959799619709573, val_Loss: 1.7206565163431375\n",
      "Epoch: 407, train_Loss: 1.7959799565920016, val_Loss: 1.7206566033572177\n",
      "Epoch: 408, train_Loss: 1.7959799644423695, val_Loss: 1.720656481972576\n",
      "Epoch: 409, train_Loss: 1.7959799621163346, val_Loss: 1.7206566033572177\n",
      "Epoch: 410, train_Loss: 1.795979963424729, val_Loss: 1.7206566033572177\n",
      "Epoch: 411, train_Loss: 1.7959799606625626, val_Loss: 1.7206566033572177\n",
      "Epoch: 412, train_Loss: 1.795979957900396, val_Loss: 1.7206565163431375\n",
      "Epoch: 413, train_Loss: 1.79597996182558, val_Loss: 1.7206565650710224\n",
      "Epoch: 414, train_Loss: 1.795979962552466, val_Loss: 1.7206565163431375\n",
      "Epoch: 415, train_Loss: 1.7959799552836069, val_Loss: 1.720656481972576\n",
      "Epoch: 416, train_Loss: 1.7959799571735102, val_Loss: 1.7206566033572177\n",
      "Epoch: 417, train_Loss: 1.795979957755019, val_Loss: 1.7206566033572177\n",
      "Epoch: 418, train_Loss: 1.7959799573188875, val_Loss: 1.720656481972576\n",
      "Epoch: 419, train_Loss: 1.795979959354168, val_Loss: 1.720656523304264\n",
      "Epoch: 420, train_Loss: 1.7959799650238781, val_Loss: 1.720656481972576\n",
      "Epoch: 421, train_Loss: 1.795979964151615, val_Loss: 1.720656523304264\n",
      "Epoch: 422, train_Loss: 1.7959799603718083, val_Loss: 1.7206565163431375\n",
      "Epoch: 423, train_Loss: 1.7959799583365277, val_Loss: 1.720656481972576\n",
      "Epoch: 424, train_Loss: 1.7959799552836069, val_Loss: 1.7206565163431375\n",
      "Epoch: 425, train_Loss: 1.795979956882756, val_Loss: 1.720656481972576\n",
      "Epoch: 426, train_Loss: 1.7959799597902997, val_Loss: 1.720656481972576\n",
      "Epoch: 427, train_Loss: 1.7959799560104928, val_Loss: 1.720656481972576\n",
      "Epoch: 428, train_Loss: 1.79597996647765, val_Loss: 1.720656523304264\n",
      "Epoch: 429, train_Loss: 1.7959799592087908, val_Loss: 1.720656481972576\n",
      "Epoch: 430, train_Loss: 1.795979961680203, val_Loss: 1.720656481972576\n",
      "Epoch: 431, train_Loss: 1.7959799542659667, val_Loss: 1.720656481972576\n",
      "Epoch: 432, train_Loss: 1.795979948160125, val_Loss: 1.7206566033572177\n",
      "Epoch: 433, train_Loss: 1.7959799586272822, val_Loss: 1.7206566033572177\n",
      "Epoch: 434, train_Loss: 1.795979964006238, val_Loss: 1.720656523304264\n",
      "Epoch: 435, train_Loss: 1.7959799576096418, val_Loss: 1.7206566033572177\n",
      "Epoch: 436, train_Loss: 1.7959799603718083, val_Loss: 1.7206565650710224\n",
      "Epoch: 437, train_Loss: 1.79597995615587, val_Loss: 1.720656481972576\n",
      "Epoch: 438, train_Loss: 1.7959799605171856, val_Loss: 1.7206566033572177\n",
      "Epoch: 439, train_Loss: 1.7959799548474753, val_Loss: 1.720656481972576\n",
      "Epoch: 440, train_Loss: 1.7959799554289841, val_Loss: 1.7206566033572177\n",
      "Epoch: 441, train_Loss: 1.795979962552466, val_Loss: 1.720656481972576\n",
      "Epoch: 442, train_Loss: 1.795979957755019, val_Loss: 1.7206566033572177\n",
      "Epoch: 443, train_Loss: 1.7959799583365277, val_Loss: 1.7206566033572177\n",
      "Epoch: 444, train_Loss: 1.795979968076799, val_Loss: 1.7206566033572177\n",
      "Epoch: 445, train_Loss: 1.7959799629885975, val_Loss: 1.7206565163431375\n",
      "Epoch: 446, train_Loss: 1.7959799544113437, val_Loss: 1.7206565163431375\n",
      "Epoch: 447, train_Loss: 1.7959799622617116, val_Loss: 1.7206566033572177\n",
      "Epoch: 448, train_Loss: 1.7959799635701064, val_Loss: 1.7206566033572177\n",
      "Epoch: 449, train_Loss: 1.795979959354168, val_Loss: 1.7206565163431375\n",
      "Epoch: 450, train_Loss: 1.795979958481905, val_Loss: 1.720656481972576\n",
      "Epoch: 451, train_Loss: 1.7959799629885975, val_Loss: 1.7206566033572177\n",
      "Epoch: 452, train_Loss: 1.7959799567373786, val_Loss: 1.7206566033572177\n",
      "Epoch: 453, train_Loss: 1.7959799626978432, val_Loss: 1.7206566033572177\n",
      "Epoch: 454, train_Loss: 1.7959799685129305, val_Loss: 1.7206565163431375\n",
      "Epoch: 455, train_Loss: 1.7959799555743612, val_Loss: 1.720656481972576\n",
      "Epoch: 456, train_Loss: 1.7959799586272822, val_Loss: 1.7206566033572177\n",
      "Epoch: 457, train_Loss: 1.7959799525214404, val_Loss: 1.720656481972576\n",
      "Epoch: 458, train_Loss: 1.7959799590634136, val_Loss: 1.720656481972576\n",
      "Epoch: 459, train_Loss: 1.7959799589180365, val_Loss: 1.7206566033572177\n",
      "Epoch: 460, train_Loss: 1.7959799573188875, val_Loss: 1.720656481972576\n",
      "Epoch: 461, train_Loss: 1.7959799589180365, val_Loss: 1.720656523304264\n",
      "Epoch: 462, train_Loss: 1.7959799637154834, val_Loss: 1.720656523304264\n",
      "Epoch: 463, train_Loss: 1.7959799603718083, val_Loss: 1.7206566033572177\n",
      "Epoch: 464, train_Loss: 1.7959799590634136, val_Loss: 1.720656481972576\n",
      "Epoch: 465, train_Loss: 1.79597995615587, val_Loss: 1.720656481972576\n",
      "Epoch: 466, train_Loss: 1.7959799523760633, val_Loss: 1.7206566033572177\n",
      "Epoch: 467, train_Loss: 1.795979961680203, val_Loss: 1.7206566033572177\n",
      "Epoch: 468, train_Loss: 1.7959799608079399, val_Loss: 1.720656481972576\n",
      "Epoch: 469, train_Loss: 1.7959799596449224, val_Loss: 1.7206566033572177\n",
      "Epoch: 470, train_Loss: 1.7959799586272822, val_Loss: 1.720656481972576\n",
      "Epoch: 471, train_Loss: 1.795979963424729, val_Loss: 1.720656481972576\n",
      "Epoch: 472, train_Loss: 1.795979961680203, val_Loss: 1.720656481972576\n",
      "Epoch: 473, train_Loss: 1.7959799590634136, val_Loss: 1.7206566033572177\n",
      "Epoch: 474, train_Loss: 1.7959799516491775, val_Loss: 1.7206566033572177\n",
      "Epoch: 475, train_Loss: 1.7959799573188875, val_Loss: 1.720656481972576\n",
      "Epoch: 476, train_Loss: 1.7959799587726593, val_Loss: 1.720656481972576\n",
      "Epoch: 477, train_Loss: 1.7959799621163346, val_Loss: 1.7206565163431375\n",
      "Epoch: 478, train_Loss: 1.7959799603718083, val_Loss: 1.7206566033572177\n",
      "Epoch: 479, train_Loss: 1.795979962407089, val_Loss: 1.720656481972576\n",
      "Epoch: 480, train_Loss: 1.7959799587726593, val_Loss: 1.720656523304264\n",
      "Epoch: 481, train_Loss: 1.7959799570281332, val_Loss: 1.720656523304264\n",
      "Epoch: 482, train_Loss: 1.7959799533937035, val_Loss: 1.720656481972576\n",
      "Epoch: 483, train_Loss: 1.7959799612440714, val_Loss: 1.7206566033572177\n",
      "Epoch: 484, train_Loss: 1.795979963424729, val_Loss: 1.720656481972576\n",
      "Epoch: 485, train_Loss: 1.7959799610986942, val_Loss: 1.7206566033572177\n",
      "Epoch: 486, train_Loss: 1.7959799576096418, val_Loss: 1.720656481972576\n",
      "Epoch: 487, train_Loss: 1.7959799567373786, val_Loss: 1.720656481972576\n",
      "Epoch: 488, train_Loss: 1.7959799536844578, val_Loss: 1.720656481972576\n",
      "Epoch: 489, train_Loss: 1.7959799520853088, val_Loss: 1.720656523304264\n",
      "Epoch: 490, train_Loss: 1.7959799645877466, val_Loss: 1.720656523304264\n",
      "Epoch: 491, train_Loss: 1.795979960226431, val_Loss: 1.7206566033572177\n",
      "Epoch: 492, train_Loss: 1.7959799570281332, val_Loss: 1.7206566033572177\n",
      "Epoch: 493, train_Loss: 1.7959799583365277, val_Loss: 1.7206566033572177\n",
      "Epoch: 494, train_Loss: 1.7959799603718083, val_Loss: 1.7206566033572177\n",
      "Epoch: 495, train_Loss: 1.7959799612440714, val_Loss: 1.7206566033572177\n",
      "Epoch: 496, train_Loss: 1.7959799615348258, val_Loss: 1.7206566033572177\n",
      "Epoch: 497, train_Loss: 1.7959799645877466, val_Loss: 1.720656523304264\n",
      "Epoch: 498, train_Loss: 1.795979959354168, val_Loss: 1.7206566033572177\n",
      "Epoch: 499, train_Loss: 1.7959799692398164, val_Loss: 1.7206566033572177\n",
      "Epoch: 500, train_Loss: 1.795979965605387, val_Loss: 1.720656523304264\n",
      "Epoch: 501, train_Loss: 1.795979962552466, val_Loss: 1.7206566033572177\n",
      "Epoch: 502, train_Loss: 1.795979952957572, val_Loss: 1.7206566033572177\n",
      "Epoch: 503, train_Loss: 1.7959799551382298, val_Loss: 1.720656523304264\n",
      "Epoch: 504, train_Loss: 1.7959799621163346, val_Loss: 1.720656523304264\n",
      "Epoch: 505, train_Loss: 1.7959799658961413, val_Loss: 1.7206565163431375\n",
      "Epoch: 506, train_Loss: 1.795979952957572, val_Loss: 1.7206565163431375\n",
      "Epoch: 507, train_Loss: 1.795979948160125, val_Loss: 1.7206565163431375\n",
      "Epoch: 508, train_Loss: 1.7959799535390808, val_Loss: 1.7206566033572177\n",
      "Epoch: 509, train_Loss: 1.795979958481905, val_Loss: 1.7206566033572177\n",
      "Epoch: 510, train_Loss: 1.795979962407089, val_Loss: 1.7206566033572177\n",
      "Epoch: 511, train_Loss: 1.795979962407089, val_Loss: 1.7206566033572177\n",
      "Epoch: 512, train_Loss: 1.795979965605387, val_Loss: 1.720656481972576\n",
      "Epoch: 513, train_Loss: 1.7959799603718083, val_Loss: 1.720656481972576\n",
      "Epoch: 514, train_Loss: 1.7959799517945545, val_Loss: 1.7206566033572177\n",
      "Epoch: 515, train_Loss: 1.7959799552836069, val_Loss: 1.7206565163431375\n",
      "Epoch: 516, train_Loss: 1.7959799653146324, val_Loss: 1.720656523304264\n",
      "Epoch: 517, train_Loss: 1.795979959354168, val_Loss: 1.7206566033572177\n",
      "Epoch: 518, train_Loss: 1.7959799570281332, val_Loss: 1.7206566033572177\n",
      "Epoch: 519, train_Loss: 1.7959799565920016, val_Loss: 1.720656481972576\n",
      "Epoch: 520, train_Loss: 1.7959799558651157, val_Loss: 1.7206565163431375\n",
      "Epoch: 521, train_Loss: 1.79597995048616, val_Loss: 1.7206566033572177\n",
      "Epoch: 522, train_Loss: 1.7959799580457734, val_Loss: 1.7206566033572177\n",
      "Epoch: 523, train_Loss: 1.7959799615348258, val_Loss: 1.7206566033572177\n",
      "Epoch: 524, train_Loss: 1.7959799599356767, val_Loss: 1.7206566033572177\n",
      "Epoch: 525, train_Loss: 1.795979952230686, val_Loss: 1.720656481972576\n",
      "Epoch: 526, train_Loss: 1.7959799590634136, val_Loss: 1.720656481972576\n",
      "Epoch: 527, train_Loss: 1.795979953829835, val_Loss: 1.7206566033572177\n",
      "Epoch: 528, train_Loss: 1.7959799637154834, val_Loss: 1.7206566033572177\n",
      "Epoch: 529, train_Loss: 1.7959799612440714, val_Loss: 1.720656523304264\n",
      "Epoch: 530, train_Loss: 1.7959799626978432, val_Loss: 1.7206566033572177\n",
      "Epoch: 531, train_Loss: 1.7959799599356767, val_Loss: 1.720656481972576\n",
      "Epoch: 532, train_Loss: 1.7959799549928526, val_Loss: 1.720656481972576\n",
      "Epoch: 533, train_Loss: 1.7959799542659667, val_Loss: 1.7206566033572177\n",
      "Epoch: 534, train_Loss: 1.7959799576096418, val_Loss: 1.7206565163431375\n",
      "Epoch: 535, train_Loss: 1.7959799549928526, val_Loss: 1.720656481972576\n",
      "Epoch: 536, train_Loss: 1.7959799535390808, val_Loss: 1.720656481972576\n",
      "Epoch: 537, train_Loss: 1.7959799525214404, val_Loss: 1.7206566033572177\n",
      "Epoch: 538, train_Loss: 1.7959799606625626, val_Loss: 1.720656481972576\n",
      "Epoch: 539, train_Loss: 1.7959799615348258, val_Loss: 1.7206566033572177\n",
      "Epoch: 540, train_Loss: 1.7959799648785009, val_Loss: 1.720656481972576\n",
      "Epoch: 541, train_Loss: 1.795979964006238, val_Loss: 1.720656481972576\n",
      "Epoch: 542, train_Loss: 1.7959799631339748, val_Loss: 1.7206566033572177\n",
      "Epoch: 543, train_Loss: 1.7959799571735102, val_Loss: 1.7206565163431375\n",
      "Epoch: 544, train_Loss: 1.7959799586272822, val_Loss: 1.720656523304264\n",
      "Epoch: 545, train_Loss: 1.7959799667684044, val_Loss: 1.720656481972576\n",
      "Epoch: 546, train_Loss: 1.7959799532483263, val_Loss: 1.720656523304264\n",
      "Epoch: 547, train_Loss: 1.7959799606625626, val_Loss: 1.7206566033572177\n",
      "Epoch: 548, train_Loss: 1.795979958481905, val_Loss: 1.720656481972576\n",
      "Epoch: 549, train_Loss: 1.795979963424729, val_Loss: 1.7206565163431375\n",
      "Epoch: 550, train_Loss: 1.7959799599356767, val_Loss: 1.720656481972576\n",
      "Epoch: 551, train_Loss: 1.7959799628432205, val_Loss: 1.7206566033572177\n",
      "Epoch: 552, train_Loss: 1.795979959354168, val_Loss: 1.7206566033572177\n",
      "Epoch: 553, train_Loss: 1.7959799635701064, val_Loss: 1.7206566033572177\n",
      "Epoch: 554, train_Loss: 1.7959799594995451, val_Loss: 1.7206566033572177\n",
      "Epoch: 555, train_Loss: 1.7959799565920016, val_Loss: 1.7206566033572177\n",
      "Epoch: 556, train_Loss: 1.7959799580457734, val_Loss: 1.720656523304264\n",
      "Epoch: 557, train_Loss: 1.7959799531029492, val_Loss: 1.720656481972576\n",
      "Epoch: 558, train_Loss: 1.7959799599356767, val_Loss: 1.7206566033572177\n",
      "Epoch: 559, train_Loss: 1.795979962407089, val_Loss: 1.7206566033572177\n",
      "Epoch: 560, train_Loss: 1.7959799551382298, val_Loss: 1.7206566033572177\n",
      "Epoch: 561, train_Loss: 1.7959799520853088, val_Loss: 1.7206566033572177\n",
      "Epoch: 562, train_Loss: 1.795979960081054, val_Loss: 1.720656481972576\n",
      "Epoch: 563, train_Loss: 1.7959799631339748, val_Loss: 1.720656523304264\n",
      "Epoch: 564, train_Loss: 1.795979964006238, val_Loss: 1.720656481972576\n",
      "Epoch: 565, train_Loss: 1.7959799497592739, val_Loss: 1.7206566033572177\n",
      "Epoch: 566, train_Loss: 1.7959799596449224, val_Loss: 1.7206565163431375\n",
      "Epoch: 567, train_Loss: 1.7959799565920016, val_Loss: 1.7206566033572177\n",
      "Epoch: 568, train_Loss: 1.7959799651692554, val_Loss: 1.7206566033572177\n",
      "Epoch: 569, train_Loss: 1.795979957755019, val_Loss: 1.720656523304264\n",
      "Epoch: 570, train_Loss: 1.795979961680203, val_Loss: 1.720656523304264\n",
      "Epoch: 571, train_Loss: 1.7959799570281332, val_Loss: 1.720656481972576\n",
      "Epoch: 572, train_Loss: 1.795979960226431, val_Loss: 1.7206566033572177\n",
      "Epoch: 573, train_Loss: 1.795979961680203, val_Loss: 1.720656481972576\n",
      "Epoch: 574, train_Loss: 1.795979960226431, val_Loss: 1.720656481972576\n",
      "Epoch: 575, train_Loss: 1.7959799644423695, val_Loss: 1.720656523304264\n",
      "Epoch: 576, train_Loss: 1.7959799621163346, val_Loss: 1.7206566033572177\n",
      "Epoch: 577, train_Loss: 1.7959799574642648, val_Loss: 1.7206566033572177\n",
      "Epoch: 578, train_Loss: 1.7959799651692554, val_Loss: 1.7206565163431375\n",
      "Epoch: 579, train_Loss: 1.79597996182558, val_Loss: 1.720656481972576\n",
      "Epoch: 580, train_Loss: 1.7959799605171856, val_Loss: 1.7206566033572177\n",
      "Epoch: 581, train_Loss: 1.7959799507769143, val_Loss: 1.7206566033572177\n",
      "Epoch: 582, train_Loss: 1.7959799638608607, val_Loss: 1.720656481972576\n",
      "Epoch: 583, train_Loss: 1.795979958481905, val_Loss: 1.720656481972576\n",
      "Epoch: 584, train_Loss: 1.7959799532483263, val_Loss: 1.720656481972576\n",
      "Epoch: 585, train_Loss: 1.7959799539752124, val_Loss: 1.720656481972576\n",
      "Epoch: 586, train_Loss: 1.7959799596449224, val_Loss: 1.720656523304264\n",
      "Epoch: 587, train_Loss: 1.7959799653146324, val_Loss: 1.7206566033572177\n",
      "Epoch: 588, train_Loss: 1.7959799574642648, val_Loss: 1.7206566033572177\n",
      "Epoch: 589, train_Loss: 1.7959799590634136, val_Loss: 1.7206565163431375\n",
      "Epoch: 590, train_Loss: 1.795979961680203, val_Loss: 1.7206564919791953\n",
      "Epoch: 591, train_Loss: 1.7959799586272822, val_Loss: 1.7206565650710224\n",
      "Epoch: 592, train_Loss: 1.7959799541205894, val_Loss: 1.720656481972576\n",
      "Epoch: 593, train_Loss: 1.7959799519399318, val_Loss: 1.7206566033572177\n",
      "Epoch: 594, train_Loss: 1.7959799612440714, val_Loss: 1.720656523304264\n",
      "Epoch: 595, train_Loss: 1.795979963279352, val_Loss: 1.7206566033572177\n",
      "Epoch: 596, train_Loss: 1.7959799477239935, val_Loss: 1.720656481972576\n",
      "Epoch: 597, train_Loss: 1.7959799480147478, val_Loss: 1.720656523304264\n",
      "Epoch: 598, train_Loss: 1.7959799642969922, val_Loss: 1.7206566033572177\n",
      "Epoch: 599, train_Loss: 1.795979963424729, val_Loss: 1.7206566033572177\n",
      "Epoch: 600, train_Loss: 1.7959799642969922, val_Loss: 1.7206566033572177\n",
      "Epoch: 601, train_Loss: 1.795979959354168, val_Loss: 1.7206566033572177\n",
      "Epoch: 602, train_Loss: 1.7959799642969922, val_Loss: 1.7206566033572177\n",
      "Epoch: 603, train_Loss: 1.795979963424729, val_Loss: 1.720656481972576\n",
      "Epoch: 604, train_Loss: 1.795979963279352, val_Loss: 1.7206566033572177\n",
      "Epoch: 605, train_Loss: 1.7959799547020983, val_Loss: 1.7206566033572177\n",
      "Epoch: 606, train_Loss: 1.7959799609533171, val_Loss: 1.720656481972576\n",
      "Epoch: 607, train_Loss: 1.795979959354168, val_Loss: 1.7206566033572177\n",
      "Epoch: 608, train_Loss: 1.7959799672045358, val_Loss: 1.720656523304264\n",
      "Epoch: 609, train_Loss: 1.7959799676406674, val_Loss: 1.7206566033572177\n",
      "Epoch: 610, train_Loss: 1.795979963279352, val_Loss: 1.720656481972576\n",
      "Epoch: 611, train_Loss: 1.7959799583365277, val_Loss: 1.720656481972576\n",
      "Epoch: 612, train_Loss: 1.7959799539752124, val_Loss: 1.720656481972576\n",
      "Epoch: 613, train_Loss: 1.7959799533937035, val_Loss: 1.720656481972576\n",
      "Epoch: 614, train_Loss: 1.795979960226431, val_Loss: 1.720656481972576\n",
      "Epoch: 615, train_Loss: 1.7959799637154834, val_Loss: 1.7206566033572177\n",
      "Epoch: 616, train_Loss: 1.795979963424729, val_Loss: 1.7206566033572177\n",
      "Epoch: 617, train_Loss: 1.7959799626978432, val_Loss: 1.7206565163431375\n",
      "Epoch: 618, train_Loss: 1.7959799597902997, val_Loss: 1.7206565163431375\n",
      "Epoch: 619, train_Loss: 1.795979962552466, val_Loss: 1.720656481972576\n",
      "Epoch: 620, train_Loss: 1.795979963424729, val_Loss: 1.720656481972576\n",
      "Epoch: 621, train_Loss: 1.7959799576096418, val_Loss: 1.720656481972576\n",
      "Epoch: 622, train_Loss: 1.7959799667684044, val_Loss: 1.720656523304264\n",
      "Epoch: 623, train_Loss: 1.795979965605387, val_Loss: 1.7206566033572177\n",
      "Epoch: 624, train_Loss: 1.795979959354168, val_Loss: 1.720656523304264\n",
      "Epoch: 625, train_Loss: 1.795979962552466, val_Loss: 1.7206566033572177\n",
      "Epoch: 626, train_Loss: 1.7959799638608607, val_Loss: 1.7206566033572177\n",
      "Epoch: 627, train_Loss: 1.7959799542659667, val_Loss: 1.7206566033572177\n",
      "Epoch: 628, train_Loss: 1.7959799658961413, val_Loss: 1.720656481972576\n",
      "Epoch: 629, train_Loss: 1.7959799580457734, val_Loss: 1.720656481972576\n",
      "Epoch: 630, train_Loss: 1.7959799626978432, val_Loss: 1.7206566033572177\n",
      "Epoch: 631, train_Loss: 1.7959799647331238, val_Loss: 1.720656481972576\n",
      "Epoch: 632, train_Loss: 1.7959799564466243, val_Loss: 1.7206566033572177\n",
      "Epoch: 633, train_Loss: 1.795979964006238, val_Loss: 1.7206566033572177\n",
      "Epoch: 634, train_Loss: 1.7959799571735102, val_Loss: 1.720656481972576\n",
      "Epoch: 635, train_Loss: 1.795979957755019, val_Loss: 1.7206565163431375\n",
      "Epoch: 636, train_Loss: 1.7959799589180365, val_Loss: 1.720656481972576\n",
      "Epoch: 637, train_Loss: 1.7959799563012473, val_Loss: 1.720656481972576\n",
      "Epoch: 638, train_Loss: 1.7959799587726593, val_Loss: 1.720656481972576\n",
      "Epoch: 639, train_Loss: 1.795979960226431, val_Loss: 1.720656523304264\n",
      "Epoch: 640, train_Loss: 1.7959799536844578, val_Loss: 1.7206566033572177\n",
      "Epoch: 641, train_Loss: 1.795979960226431, val_Loss: 1.720656523304264\n",
      "Epoch: 642, train_Loss: 1.795979967931422, val_Loss: 1.7206565163431375\n",
      "Epoch: 643, train_Loss: 1.7959799594995451, val_Loss: 1.720656481972576\n",
      "Epoch: 644, train_Loss: 1.7959799567373786, val_Loss: 1.7206565163431375\n",
      "Epoch: 645, train_Loss: 1.7959799542659667, val_Loss: 1.7206566033572177\n",
      "Epoch: 646, train_Loss: 1.7959799570281332, val_Loss: 1.7206566033572177\n",
      "Epoch: 647, train_Loss: 1.7959799612440714, val_Loss: 1.7206565163431375\n",
      "Epoch: 648, train_Loss: 1.7959799549928526, val_Loss: 1.7206566033572177\n",
      "Epoch: 649, train_Loss: 1.795979964006238, val_Loss: 1.720656481972576\n",
      "Epoch: 650, train_Loss: 1.795979954556721, val_Loss: 1.7206565163431375\n",
      "Epoch: 651, train_Loss: 1.7959799583365277, val_Loss: 1.7206566033572177\n",
      "Epoch: 652, train_Loss: 1.7959799599356767, val_Loss: 1.7206565163431375\n",
      "Epoch: 653, train_Loss: 1.7959799605171856, val_Loss: 1.7206565163431375\n",
      "Epoch: 654, train_Loss: 1.7959799642969922, val_Loss: 1.720656481972576\n",
      "Epoch: 655, train_Loss: 1.795979959354168, val_Loss: 1.7206566033572177\n",
      "Epoch: 656, train_Loss: 1.795979963279352, val_Loss: 1.7206566033572177\n",
      "Epoch: 657, train_Loss: 1.7959799650238781, val_Loss: 1.720656523304264\n",
      "Epoch: 658, train_Loss: 1.7959799554289841, val_Loss: 1.7206566033572177\n",
      "Epoch: 659, train_Loss: 1.795979959354168, val_Loss: 1.7206566033572177\n",
      "Epoch: 660, train_Loss: 1.7959799564466243, val_Loss: 1.7206566033572177\n",
      "Epoch: 661, train_Loss: 1.7959799606625626, val_Loss: 1.720656481972576\n",
      "Epoch: 662, train_Loss: 1.795979963279352, val_Loss: 1.7206566033572177\n",
      "Epoch: 663, train_Loss: 1.7959799586272822, val_Loss: 1.7206566033572177\n",
      "Epoch: 664, train_Loss: 1.7959799610986942, val_Loss: 1.720656523304264\n",
      "Epoch: 665, train_Loss: 1.795979958481905, val_Loss: 1.7206566033572177\n",
      "Epoch: 666, train_Loss: 1.795979963424729, val_Loss: 1.720656481972576\n",
      "Epoch: 667, train_Loss: 1.7959799621163346, val_Loss: 1.7206566033572177\n",
      "Epoch: 668, train_Loss: 1.7959799592087908, val_Loss: 1.720656481972576\n",
      "Epoch: 669, train_Loss: 1.795979960081054, val_Loss: 1.720656523304264\n",
      "Epoch: 670, train_Loss: 1.7959799589180365, val_Loss: 1.720656523304264\n",
      "Epoch: 671, train_Loss: 1.7959799542659667, val_Loss: 1.720656523304264\n",
      "Epoch: 672, train_Loss: 1.7959799635701064, val_Loss: 1.720656481972576\n",
      "Epoch: 673, train_Loss: 1.7959799560104928, val_Loss: 1.7206565163431375\n",
      "Epoch: 674, train_Loss: 1.7959799666230272, val_Loss: 1.7206565163431375\n",
      "Epoch: 675, train_Loss: 1.795979954556721, val_Loss: 1.720656481972576\n",
      "Epoch: 676, train_Loss: 1.7959799501954055, val_Loss: 1.720656481972576\n",
      "Epoch: 677, train_Loss: 1.7959799576096418, val_Loss: 1.720656481972576\n",
      "Epoch: 678, train_Loss: 1.795979957900396, val_Loss: 1.720656523304264\n",
      "Epoch: 679, train_Loss: 1.7959799587726593, val_Loss: 1.720656481972576\n",
      "Epoch: 680, train_Loss: 1.7959799650238781, val_Loss: 1.720656523304264\n",
      "Epoch: 681, train_Loss: 1.7959799612440714, val_Loss: 1.7206566033572177\n",
      "Epoch: 682, train_Loss: 1.7959799637154834, val_Loss: 1.7206566033572177\n",
      "Epoch: 683, train_Loss: 1.7959799647331238, val_Loss: 1.7206566033572177\n",
      "Epoch: 684, train_Loss: 1.7959799621163346, val_Loss: 1.720656481972576\n",
      "Epoch: 685, train_Loss: 1.7959799565920016, val_Loss: 1.720656481972576\n",
      "Epoch: 686, train_Loss: 1.7959799605171856, val_Loss: 1.720656481972576\n",
      "Epoch: 687, train_Loss: 1.7959799609533171, val_Loss: 1.7206566033572177\n",
      "Epoch: 688, train_Loss: 1.7959799647331238, val_Loss: 1.7206566033572177\n",
      "Epoch: 689, train_Loss: 1.795979954556721, val_Loss: 1.7206565163431375\n",
      "Epoch: 690, train_Loss: 1.7959799581911506, val_Loss: 1.720656523304264\n",
      "Epoch: 691, train_Loss: 1.7959799571735102, val_Loss: 1.7206566033572177\n",
      "Epoch: 692, train_Loss: 1.7959799695305707, val_Loss: 1.720656481972576\n",
      "Epoch: 693, train_Loss: 1.7959799596449224, val_Loss: 1.720656481972576\n",
      "Epoch: 694, train_Loss: 1.795979956882756, val_Loss: 1.720656481972576\n",
      "Epoch: 695, train_Loss: 1.7959799622617116, val_Loss: 1.720656523304264\n",
      "Epoch: 696, train_Loss: 1.7959799621163346, val_Loss: 1.720656481972576\n",
      "Epoch: 697, train_Loss: 1.7959799605171856, val_Loss: 1.7206566033572177\n",
      "Epoch: 698, train_Loss: 1.795979963279352, val_Loss: 1.7206565163431375\n",
      "Epoch: 699, train_Loss: 1.7959799563012473, val_Loss: 1.7206566033572177\n",
      "Epoch: 700, train_Loss: 1.79597996182558, val_Loss: 1.7206566033572177\n",
      "Epoch: 701, train_Loss: 1.7959799599356767, val_Loss: 1.7206566033572177\n",
      "Epoch: 702, train_Loss: 1.7959799587726593, val_Loss: 1.7206566033572177\n",
      "Epoch: 703, train_Loss: 1.795979963424729, val_Loss: 1.7206566033572177\n",
      "Epoch: 704, train_Loss: 1.7959799574642648, val_Loss: 1.7206566033572177\n",
      "Epoch: 705, train_Loss: 1.7959799516491775, val_Loss: 1.7206566033572177\n",
      "Epoch: 706, train_Loss: 1.795979960081054, val_Loss: 1.7206566033572177\n",
      "Epoch: 707, train_Loss: 1.7959799626978432, val_Loss: 1.720656481972576\n",
      "Epoch: 708, train_Loss: 1.7959799574642648, val_Loss: 1.720656481972576\n",
      "Epoch: 709, train_Loss: 1.795979961680203, val_Loss: 1.720656481972576\n",
      "Epoch: 710, train_Loss: 1.7959799613894485, val_Loss: 1.7206566033572177\n",
      "Epoch: 711, train_Loss: 1.795979958481905, val_Loss: 1.720656481972576\n",
      "Epoch: 712, train_Loss: 1.7959799609533171, val_Loss: 1.720656481972576\n",
      "Epoch: 713, train_Loss: 1.7959799648785009, val_Loss: 1.7206566033572177\n",
      "Epoch: 714, train_Loss: 1.7959799500500284, val_Loss: 1.720656481972576\n",
      "Epoch: 715, train_Loss: 1.7959799605171856, val_Loss: 1.7206566033572177\n",
      "Epoch: 716, train_Loss: 1.7959799613894485, val_Loss: 1.7206566033572177\n",
      "Epoch: 717, train_Loss: 1.7959799648785009, val_Loss: 1.7206566033572177\n",
      "Epoch: 718, train_Loss: 1.795979962552466, val_Loss: 1.7206566033572177\n",
      "Epoch: 719, train_Loss: 1.7959799612440714, val_Loss: 1.7206565163431375\n",
      "Epoch: 720, train_Loss: 1.795979960081054, val_Loss: 1.720656481972576\n",
      "Epoch: 721, train_Loss: 1.7959799539752124, val_Loss: 1.7206566033572177\n",
      "Epoch: 722, train_Loss: 1.795979958481905, val_Loss: 1.720656481972576\n",
      "Epoch: 723, train_Loss: 1.795979959354168, val_Loss: 1.7206565650710224\n",
      "Epoch: 724, train_Loss: 1.795979956882756, val_Loss: 1.7206566033572177\n",
      "Epoch: 725, train_Loss: 1.7959799590634136, val_Loss: 1.7206566033572177\n",
      "Epoch: 726, train_Loss: 1.79597995615587, val_Loss: 1.7206566033572177\n",
      "Epoch: 727, train_Loss: 1.7959799516491775, val_Loss: 1.7206566033572177\n",
      "Epoch: 728, train_Loss: 1.795979957755019, val_Loss: 1.720656481972576\n",
      "Epoch: 729, train_Loss: 1.7959799596449224, val_Loss: 1.720656481972576\n",
      "Epoch: 730, train_Loss: 1.795979961680203, val_Loss: 1.720656481972576\n",
      "Epoch: 731, train_Loss: 1.7959799612440714, val_Loss: 1.720656523304264\n",
      "Epoch: 732, train_Loss: 1.7959799708389654, val_Loss: 1.7206566033572177\n",
      "Epoch: 733, train_Loss: 1.7959799594995451, val_Loss: 1.720656523304264\n",
      "Epoch: 734, train_Loss: 1.7959799557197385, val_Loss: 1.720656481972576\n",
      "Epoch: 735, train_Loss: 1.7959799570281332, val_Loss: 1.7206565163431375\n",
      "Epoch: 736, train_Loss: 1.795979956882756, val_Loss: 1.720656481972576\n",
      "Epoch: 737, train_Loss: 1.7959799570281332, val_Loss: 1.720656481972576\n",
      "Epoch: 738, train_Loss: 1.7959799583365277, val_Loss: 1.7206565163431375\n",
      "Epoch: 739, train_Loss: 1.7959799589180365, val_Loss: 1.7206565163431375\n",
      "Epoch: 740, train_Loss: 1.7959799532483263, val_Loss: 1.7206565163431375\n",
      "Epoch: 741, train_Loss: 1.7959799549928526, val_Loss: 1.7206565163431375\n",
      "Epoch: 742, train_Loss: 1.7959799626978432, val_Loss: 1.7206566033572177\n",
      "Epoch: 743, train_Loss: 1.7959799619709573, val_Loss: 1.720656481972576\n",
      "Epoch: 744, train_Loss: 1.7959799570281332, val_Loss: 1.7206566033572177\n",
      "Epoch: 745, train_Loss: 1.7959799573188875, val_Loss: 1.7206566033572177\n",
      "Epoch: 746, train_Loss: 1.7959799628432205, val_Loss: 1.7206566033572177\n",
      "Epoch: 747, train_Loss: 1.7959799599356767, val_Loss: 1.7206566033572177\n",
      "Epoch: 748, train_Loss: 1.7959799509222913, val_Loss: 1.7206566033572177\n",
      "Epoch: 749, train_Loss: 1.795979949032388, val_Loss: 1.7206566033572177\n",
      "Epoch: 750, train_Loss: 1.7959799626978432, val_Loss: 1.7206566033572177\n",
      "Epoch: 751, train_Loss: 1.7959799576096418, val_Loss: 1.7206565163431375\n",
      "Epoch: 752, train_Loss: 1.7959799596449224, val_Loss: 1.7206566033572177\n",
      "Epoch: 753, train_Loss: 1.7959799686583078, val_Loss: 1.720656523304264\n",
      "Epoch: 754, train_Loss: 1.7959799621163346, val_Loss: 1.720656481972576\n",
      "Epoch: 755, train_Loss: 1.7959799622617116, val_Loss: 1.720656523304264\n",
      "Epoch: 756, train_Loss: 1.79597995615587, val_Loss: 1.720656481972576\n",
      "Epoch: 757, train_Loss: 1.795979961680203, val_Loss: 1.720656481972576\n",
      "Epoch: 758, train_Loss: 1.7959799573188875, val_Loss: 1.7206565163431375\n",
      "Epoch: 759, train_Loss: 1.7959799605171856, val_Loss: 1.720656481972576\n",
      "Epoch: 760, train_Loss: 1.795979956882756, val_Loss: 1.7206566033572177\n",
      "Epoch: 761, train_Loss: 1.7959799576096418, val_Loss: 1.720656481972576\n",
      "Epoch: 762, train_Loss: 1.7959799571735102, val_Loss: 1.7206565163431375\n",
      "Epoch: 763, train_Loss: 1.795979957900396, val_Loss: 1.7206566033572177\n",
      "Epoch: 764, train_Loss: 1.7959799587726593, val_Loss: 1.7206565163431375\n",
      "Epoch: 765, train_Loss: 1.7959799638608607, val_Loss: 1.720656481972576\n",
      "Epoch: 766, train_Loss: 1.7959799612440714, val_Loss: 1.7206565163431375\n",
      "Epoch: 767, train_Loss: 1.795979958481905, val_Loss: 1.7206566033572177\n",
      "Epoch: 768, train_Loss: 1.795979957755019, val_Loss: 1.7206565163431375\n",
      "Epoch: 769, train_Loss: 1.7959799674952903, val_Loss: 1.7206566033572177\n",
      "Epoch: 770, train_Loss: 1.795979963424729, val_Loss: 1.7206566033572177\n",
      "Epoch: 771, train_Loss: 1.7959799650238781, val_Loss: 1.720656481972576\n",
      "Epoch: 772, train_Loss: 1.7959799622617116, val_Loss: 1.7206565163431375\n",
      "Epoch: 773, train_Loss: 1.7959799555743612, val_Loss: 1.7206565163431375\n",
      "Epoch: 774, train_Loss: 1.7959799622617116, val_Loss: 1.720656481972576\n",
      "Epoch: 775, train_Loss: 1.7959799622617116, val_Loss: 1.7206566033572177\n",
      "Epoch: 776, train_Loss: 1.795979957755019, val_Loss: 1.7206566033572177\n",
      "Epoch: 777, train_Loss: 1.795979954556721, val_Loss: 1.7206566033572177\n",
      "Epoch: 778, train_Loss: 1.795979952957572, val_Loss: 1.7206566033572177\n",
      "Epoch: 779, train_Loss: 1.7959799637154834, val_Loss: 1.7206566033572177\n",
      "Epoch: 780, train_Loss: 1.7959799525214404, val_Loss: 1.720656481972576\n",
      "Epoch: 781, train_Loss: 1.7959799621163346, val_Loss: 1.7206566033572177\n",
      "Epoch: 782, train_Loss: 1.7959799645877466, val_Loss: 1.7206566033572177\n",
      "Epoch: 783, train_Loss: 1.7959799515038002, val_Loss: 1.720656523304264\n",
      "Epoch: 784, train_Loss: 1.79597996647765, val_Loss: 1.720656523304264\n",
      "Epoch: 785, train_Loss: 1.7959799567373786, val_Loss: 1.7206566033572177\n",
      "Epoch: 786, train_Loss: 1.7959799609533171, val_Loss: 1.7206566033572177\n",
      "Epoch: 787, train_Loss: 1.7959799512130459, val_Loss: 1.720656523304264\n",
      "Epoch: 788, train_Loss: 1.7959799592087908, val_Loss: 1.7206566033572177\n",
      "Epoch: 789, train_Loss: 1.7959799535390808, val_Loss: 1.7206566033572177\n",
      "Epoch: 790, train_Loss: 1.7959799635701064, val_Loss: 1.720656523304264\n",
      "Epoch: 791, train_Loss: 1.7959799650238781, val_Loss: 1.720656481972576\n",
      "Epoch: 792, train_Loss: 1.7959799567373786, val_Loss: 1.7206566033572177\n",
      "Epoch: 793, train_Loss: 1.7959799666230272, val_Loss: 1.7206566033572177\n",
      "Epoch: 794, train_Loss: 1.7959799544113437, val_Loss: 1.720656481972576\n",
      "Epoch: 795, train_Loss: 1.7959799645877466, val_Loss: 1.7206565163431375\n",
      "Epoch: 796, train_Loss: 1.7959799592087908, val_Loss: 1.7206566033572177\n",
      "Epoch: 797, train_Loss: 1.7959799558651157, val_Loss: 1.7206566033572177\n",
      "Epoch: 798, train_Loss: 1.7959799612440714, val_Loss: 1.720656481972576\n",
      "Epoch: 799, train_Loss: 1.795979964006238, val_Loss: 1.7206565163431375\n",
      "Epoch: 800, train_Loss: 1.7959799548474753, val_Loss: 1.7206565163431375\n",
      "Epoch: 801, train_Loss: 1.795979956882756, val_Loss: 1.7206565163431375\n",
      "Epoch: 802, train_Loss: 1.7959799573188875, val_Loss: 1.720656481972576\n",
      "Epoch: 803, train_Loss: 1.7959799608079399, val_Loss: 1.7206566033572177\n",
      "Epoch: 804, train_Loss: 1.7959799599356767, val_Loss: 1.7206566033572177\n",
      "Epoch: 805, train_Loss: 1.795979957900396, val_Loss: 1.7206566033572177\n",
      "Epoch: 806, train_Loss: 1.7959799596449224, val_Loss: 1.720656523304264\n",
      "Epoch: 807, train_Loss: 1.7959799592087908, val_Loss: 1.7206566033572177\n",
      "Epoch: 808, train_Loss: 1.7959799590634136, val_Loss: 1.720656481972576\n",
      "Epoch: 809, train_Loss: 1.7959799587726593, val_Loss: 1.7206565163431375\n",
      "Epoch: 810, train_Loss: 1.795979960081054, val_Loss: 1.720656523304264\n",
      "Epoch: 811, train_Loss: 1.7959799596449224, val_Loss: 1.7206566033572177\n",
      "Epoch: 812, train_Loss: 1.7959799615348258, val_Loss: 1.7206566033572177\n",
      "Epoch: 813, train_Loss: 1.7959799523760633, val_Loss: 1.720656481972576\n",
      "Epoch: 814, train_Loss: 1.7959799573188875, val_Loss: 1.720656481972576\n",
      "Epoch: 815, train_Loss: 1.7959799619709573, val_Loss: 1.720656523304264\n",
      "Epoch: 816, train_Loss: 1.7959799594995451, val_Loss: 1.720656523304264\n",
      "Epoch: 817, train_Loss: 1.7959799583365277, val_Loss: 1.7206565163431375\n",
      "Epoch: 818, train_Loss: 1.795979962552466, val_Loss: 1.720656481972576\n",
      "Epoch: 819, train_Loss: 1.795979961680203, val_Loss: 1.7206566033572177\n",
      "Epoch: 820, train_Loss: 1.7959799613894485, val_Loss: 1.7206566033572177\n",
      "Epoch: 821, train_Loss: 1.7959799619709573, val_Loss: 1.7206565163431375\n",
      "Epoch: 822, train_Loss: 1.795979960081054, val_Loss: 1.7206566033572177\n",
      "Epoch: 823, train_Loss: 1.7959799663322729, val_Loss: 1.7206566033572177\n",
      "Epoch: 824, train_Loss: 1.7959799494685196, val_Loss: 1.7206565650710224\n",
      "Epoch: 825, train_Loss: 1.7959799564466243, val_Loss: 1.7206565163431375\n",
      "Epoch: 826, train_Loss: 1.7959799606625626, val_Loss: 1.7206565163431375\n",
      "Epoch: 827, train_Loss: 1.7959799548474753, val_Loss: 1.7206566033572177\n",
      "Epoch: 828, train_Loss: 1.795979960226431, val_Loss: 1.720656481972576\n",
      "Epoch: 829, train_Loss: 1.7959799580457734, val_Loss: 1.7206566033572177\n",
      "Epoch: 830, train_Loss: 1.79597996182558, val_Loss: 1.720656481972576\n",
      "Epoch: 831, train_Loss: 1.7959799667684044, val_Loss: 1.720656523304264\n",
      "Epoch: 832, train_Loss: 1.795979952230686, val_Loss: 1.720656481972576\n",
      "Epoch: 833, train_Loss: 1.7959799580457734, val_Loss: 1.7206566033572177\n",
      "Epoch: 834, train_Loss: 1.795979958481905, val_Loss: 1.7206566033572177\n",
      "Epoch: 835, train_Loss: 1.7959799573188875, val_Loss: 1.7206566033572177\n",
      "Epoch: 836, train_Loss: 1.7959799570281332, val_Loss: 1.7206566033572177\n",
      "Epoch: 837, train_Loss: 1.7959799610986942, val_Loss: 1.7206565163431375\n",
      "Epoch: 838, train_Loss: 1.795979962407089, val_Loss: 1.720656481972576\n",
      "Epoch: 839, train_Loss: 1.7959799510676686, val_Loss: 1.7206565163431375\n",
      "Epoch: 840, train_Loss: 1.7959799570281332, val_Loss: 1.7206566033572177\n",
      "Epoch: 841, train_Loss: 1.7959799667684044, val_Loss: 1.7206566033572177\n",
      "Epoch: 842, train_Loss: 1.7959799635701064, val_Loss: 1.7206566033572177\n",
      "Epoch: 843, train_Loss: 1.79597995615587, val_Loss: 1.720656481972576\n",
      "Epoch: 844, train_Loss: 1.7959799519399318, val_Loss: 1.720656523304264\n",
      "Epoch: 845, train_Loss: 1.795979962552466, val_Loss: 1.720656523304264\n",
      "Epoch: 846, train_Loss: 1.7959799563012473, val_Loss: 1.720656523304264\n",
      "Epoch: 847, train_Loss: 1.795979957900396, val_Loss: 1.720656481972576\n",
      "Epoch: 848, train_Loss: 1.7959799596449224, val_Loss: 1.7206566033572177\n",
      "Epoch: 849, train_Loss: 1.7959799648785009, val_Loss: 1.7206565163431375\n",
      "Epoch: 850, train_Loss: 1.7959799613894485, val_Loss: 1.7206565650710224\n",
      "Epoch: 851, train_Loss: 1.7959799589180365, val_Loss: 1.7206565163431375\n",
      "Epoch: 852, train_Loss: 1.7959799587726593, val_Loss: 1.7206565163431375\n",
      "Epoch: 853, train_Loss: 1.7959799608079399, val_Loss: 1.720656481972576\n",
      "Epoch: 854, train_Loss: 1.795979957755019, val_Loss: 1.720656481972576\n",
      "Epoch: 855, train_Loss: 1.7959799573188875, val_Loss: 1.720656481972576\n",
      "Epoch: 856, train_Loss: 1.7959799574642648, val_Loss: 1.7206566033572177\n",
      "Epoch: 857, train_Loss: 1.795979957900396, val_Loss: 1.7206566033572177\n",
      "Epoch: 858, train_Loss: 1.795979964151615, val_Loss: 1.720656523304264\n",
      "Epoch: 859, train_Loss: 1.7959799663322729, val_Loss: 1.7206566033572177\n",
      "Epoch: 860, train_Loss: 1.7959799549928526, val_Loss: 1.720656481972576\n",
      "Epoch: 861, train_Loss: 1.79597996182558, val_Loss: 1.720656481972576\n",
      "Epoch: 862, train_Loss: 1.7959799658961413, val_Loss: 1.7206566033572177\n",
      "Epoch: 863, train_Loss: 1.7959799597902997, val_Loss: 1.7206566033572177\n",
      "Epoch: 864, train_Loss: 1.7959799597902997, val_Loss: 1.720656481972576\n",
      "Epoch: 865, train_Loss: 1.7959799567373786, val_Loss: 1.7206565163431375\n",
      "Epoch: 866, train_Loss: 1.7959799597902997, val_Loss: 1.720656523304264\n",
      "Epoch: 867, train_Loss: 1.7959799596449224, val_Loss: 1.720656523304264\n",
      "Epoch: 868, train_Loss: 1.795979960081054, val_Loss: 1.7206565163431375\n",
      "Epoch: 869, train_Loss: 1.7959799503407827, val_Loss: 1.720656481972576\n",
      "Epoch: 870, train_Loss: 1.7959799539752124, val_Loss: 1.7206566033572177\n",
      "Epoch: 871, train_Loss: 1.7959799698213252, val_Loss: 1.7206566033572177\n",
      "Epoch: 872, train_Loss: 1.7959799606625626, val_Loss: 1.7206566033572177\n",
      "Epoch: 873, train_Loss: 1.7959799637154834, val_Loss: 1.7206566033572177\n",
      "Epoch: 874, train_Loss: 1.795979967349913, val_Loss: 1.7206565163431375\n",
      "Epoch: 875, train_Loss: 1.7959799583365277, val_Loss: 1.7206565163431375\n",
      "Epoch: 876, train_Loss: 1.7959799558651157, val_Loss: 1.7206565163431375\n",
      "Epoch: 877, train_Loss: 1.7959799599356767, val_Loss: 1.7206566033572177\n",
      "Epoch: 878, train_Loss: 1.7959799609533171, val_Loss: 1.7206566033572177\n",
      "Epoch: 879, train_Loss: 1.7959799571735102, val_Loss: 1.7206566033572177\n",
      "Epoch: 880, train_Loss: 1.7959799609533171, val_Loss: 1.7206566033572177\n",
      "Epoch: 881, train_Loss: 1.7959799594995451, val_Loss: 1.720656481972576\n",
      "Epoch: 882, train_Loss: 1.795979964006238, val_Loss: 1.720656523304264\n",
      "Epoch: 883, train_Loss: 1.7959799702574568, val_Loss: 1.720656523304264\n",
      "Epoch: 884, train_Loss: 1.7959799608079399, val_Loss: 1.7206566033572177\n",
      "Epoch: 885, train_Loss: 1.7959799644423695, val_Loss: 1.7206566033572177\n",
      "Epoch: 886, train_Loss: 1.7959799567373786, val_Loss: 1.720656481972576\n",
      "Epoch: 887, train_Loss: 1.7959799592087908, val_Loss: 1.7206566033572177\n",
      "Epoch: 888, train_Loss: 1.7959799583365277, val_Loss: 1.7206565163431375\n",
      "Epoch: 889, train_Loss: 1.7959799651692554, val_Loss: 1.720656481972576\n",
      "Epoch: 890, train_Loss: 1.7959799557197385, val_Loss: 1.7206565163431375\n",
      "Epoch: 891, train_Loss: 1.7959799547020983, val_Loss: 1.7206566033572177\n",
      "Epoch: 892, train_Loss: 1.7959799570281332, val_Loss: 1.7206565163431375\n",
      "Epoch: 893, train_Loss: 1.7959799542659667, val_Loss: 1.720656523304264\n",
      "Epoch: 894, train_Loss: 1.795979965605387, val_Loss: 1.720656481972576\n",
      "Epoch: 895, train_Loss: 1.7959799612440714, val_Loss: 1.7206566033572177\n",
      "Epoch: 896, train_Loss: 1.795979957755019, val_Loss: 1.7206565163431375\n",
      "Epoch: 897, train_Loss: 1.795979962407089, val_Loss: 1.7206565163431375\n",
      "Epoch: 898, train_Loss: 1.7959799552836069, val_Loss: 1.720656481972576\n",
      "Epoch: 899, train_Loss: 1.7959799551382298, val_Loss: 1.720656481972576\n",
      "Epoch: 900, train_Loss: 1.7959799594995451, val_Loss: 1.7206566033572177\n",
      "Epoch: 901, train_Loss: 1.795979965605387, val_Loss: 1.720656481972576\n",
      "Epoch: 902, train_Loss: 1.7959799613894485, val_Loss: 1.720656523304264\n",
      "Epoch: 903, train_Loss: 1.7959799589180365, val_Loss: 1.720656523304264\n",
      "Epoch: 904, train_Loss: 1.7959799612440714, val_Loss: 1.720656523304264\n",
      "Epoch: 905, train_Loss: 1.7959799560104928, val_Loss: 1.720656481972576\n",
      "Epoch: 906, train_Loss: 1.795979953829835, val_Loss: 1.7206565650710224\n",
      "Epoch: 907, train_Loss: 1.7959799499046512, val_Loss: 1.720656481972576\n",
      "Epoch: 908, train_Loss: 1.7959799613894485, val_Loss: 1.720656481972576\n",
      "Epoch: 909, train_Loss: 1.7959799509222913, val_Loss: 1.7206566033572177\n",
      "Epoch: 910, train_Loss: 1.7959799674952903, val_Loss: 1.720656481972576\n",
      "Epoch: 911, train_Loss: 1.7959799589180365, val_Loss: 1.720656481972576\n",
      "Epoch: 912, train_Loss: 1.795979962407089, val_Loss: 1.7206566033572177\n",
      "Epoch: 913, train_Loss: 1.7959799677860446, val_Loss: 1.7206566033572177\n",
      "Epoch: 914, train_Loss: 1.7959799621163346, val_Loss: 1.720656481972576\n",
      "Epoch: 915, train_Loss: 1.7959799589180365, val_Loss: 1.720656481972576\n",
      "Epoch: 916, train_Loss: 1.7959799658961413, val_Loss: 1.720656481972576\n",
      "Epoch: 917, train_Loss: 1.7959799567373786, val_Loss: 1.720656481972576\n",
      "Epoch: 918, train_Loss: 1.7959799501954055, val_Loss: 1.720656481972576\n",
      "Epoch: 919, train_Loss: 1.7959799542659667, val_Loss: 1.720656481972576\n",
      "Epoch: 920, train_Loss: 1.7959799547020983, val_Loss: 1.7206566033572177\n",
      "Epoch: 921, train_Loss: 1.7959799608079399, val_Loss: 1.7206566033572177\n",
      "Epoch: 922, train_Loss: 1.7959799672045358, val_Loss: 1.720656481972576\n",
      "Epoch: 923, train_Loss: 1.7959799586272822, val_Loss: 1.7206565163431375\n",
      "Epoch: 924, train_Loss: 1.7959799626978432, val_Loss: 1.720656481972576\n",
      "Epoch: 925, train_Loss: 1.795979964006238, val_Loss: 1.7206566033572177\n",
      "Epoch: 926, train_Loss: 1.795979960081054, val_Loss: 1.720656523304264\n",
      "Epoch: 927, train_Loss: 1.7959799576096418, val_Loss: 1.7206565163431375\n",
      "Epoch: 928, train_Loss: 1.7959799535390808, val_Loss: 1.7206565163431375\n",
      "Epoch: 929, train_Loss: 1.7959799573188875, val_Loss: 1.7206565163431375\n",
      "Epoch: 930, train_Loss: 1.7959799597902997, val_Loss: 1.7206566033572177\n",
      "Epoch: 931, train_Loss: 1.7959799608079399, val_Loss: 1.7206566033572177\n",
      "Epoch: 932, train_Loss: 1.7959799557197385, val_Loss: 1.7206566033572177\n",
      "Epoch: 933, train_Loss: 1.795979962552466, val_Loss: 1.7206566033572177\n",
      "Epoch: 934, train_Loss: 1.795979961680203, val_Loss: 1.720656481972576\n",
      "Epoch: 935, train_Loss: 1.7959799590634136, val_Loss: 1.7206566033572177\n",
      "Epoch: 936, train_Loss: 1.795979963279352, val_Loss: 1.720656523304264\n",
      "Epoch: 937, train_Loss: 1.7959799621163346, val_Loss: 1.720656523304264\n",
      "Epoch: 938, train_Loss: 1.7959799637154834, val_Loss: 1.720656481972576\n",
      "Epoch: 939, train_Loss: 1.7959799642969922, val_Loss: 1.720656481972576\n",
      "Epoch: 940, train_Loss: 1.7959799612440714, val_Loss: 1.720656481972576\n",
      "Epoch: 941, train_Loss: 1.795979958481905, val_Loss: 1.7206565163431375\n",
      "Epoch: 942, train_Loss: 1.795979963279352, val_Loss: 1.7206565163431375\n",
      "Epoch: 943, train_Loss: 1.795979944961827, val_Loss: 1.7206565163431375\n",
      "Epoch: 944, train_Loss: 1.795979951358423, val_Loss: 1.720656481972576\n",
      "Epoch: 945, train_Loss: 1.7959799608079399, val_Loss: 1.720656481972576\n",
      "Epoch: 946, train_Loss: 1.7959799542659667, val_Loss: 1.720656481972576\n",
      "Epoch: 947, train_Loss: 1.7959799603718083, val_Loss: 1.7206566033572177\n",
      "Epoch: 948, train_Loss: 1.7959799587726593, val_Loss: 1.7206566033572177\n",
      "Epoch: 949, train_Loss: 1.795979960081054, val_Loss: 1.720656481972576\n",
      "Epoch: 950, train_Loss: 1.7959799603718083, val_Loss: 1.7206566033572177\n",
      "Epoch: 951, train_Loss: 1.7959799597902997, val_Loss: 1.720656481972576\n",
      "Epoch: 952, train_Loss: 1.7959799574642648, val_Loss: 1.720656481972576\n",
      "Epoch: 953, train_Loss: 1.7959799507769143, val_Loss: 1.720656481972576\n",
      "Epoch: 954, train_Loss: 1.7959799542659667, val_Loss: 1.720656523304264\n",
      "Epoch: 955, train_Loss: 1.7959799642969922, val_Loss: 1.720656523304264\n",
      "Epoch: 956, train_Loss: 1.7959799642969922, val_Loss: 1.720656523304264\n",
      "Epoch: 957, train_Loss: 1.7959799651692554, val_Loss: 1.7206565163431375\n",
      "Epoch: 958, train_Loss: 1.7959799586272822, val_Loss: 1.7206565163431375\n",
      "Epoch: 959, train_Loss: 1.7959799605171856, val_Loss: 1.720656481972576\n",
      "Epoch: 960, train_Loss: 1.7959799645877466, val_Loss: 1.720656481972576\n",
      "Epoch: 961, train_Loss: 1.7959799547020983, val_Loss: 1.720656481972576\n",
      "Epoch: 962, train_Loss: 1.7959799597902997, val_Loss: 1.720656481972576\n",
      "Epoch: 963, train_Loss: 1.7959799580457734, val_Loss: 1.7206566033572177\n",
      "Epoch: 964, train_Loss: 1.795979967349913, val_Loss: 1.7206566033572177\n",
      "Epoch: 965, train_Loss: 1.7959799590634136, val_Loss: 1.7206566033572177\n",
      "Epoch: 966, train_Loss: 1.7959799583365277, val_Loss: 1.720656481972576\n",
      "Epoch: 967, train_Loss: 1.7959799547020983, val_Loss: 1.7206566033572177\n",
      "Epoch: 968, train_Loss: 1.7959799525214404, val_Loss: 1.720656481972576\n",
      "Epoch: 969, train_Loss: 1.7959799526668176, val_Loss: 1.7206565650710224\n",
      "Epoch: 970, train_Loss: 1.7959799597902997, val_Loss: 1.7206566033572177\n",
      "Epoch: 971, train_Loss: 1.7959799574642648, val_Loss: 1.7206566033572177\n",
      "Epoch: 972, train_Loss: 1.7959799603718083, val_Loss: 1.7206565163431375\n",
      "Epoch: 973, train_Loss: 1.7959799586272822, val_Loss: 1.7206566033572177\n",
      "Epoch: 974, train_Loss: 1.7959799644423695, val_Loss: 1.720656481972576\n",
      "Epoch: 975, train_Loss: 1.7959799563012473, val_Loss: 1.7206565163431375\n",
      "Epoch: 976, train_Loss: 1.795979957900396, val_Loss: 1.720656481972576\n",
      "Epoch: 977, train_Loss: 1.7959799512130459, val_Loss: 1.7206566033572177\n",
      "Epoch: 978, train_Loss: 1.7959799615348258, val_Loss: 1.7206566033572177\n",
      "Epoch: 979, train_Loss: 1.795979962552466, val_Loss: 1.7206566033572177\n",
      "Epoch: 980, train_Loss: 1.795979957755019, val_Loss: 1.7206564919791953\n",
      "Epoch: 981, train_Loss: 1.7959799563012473, val_Loss: 1.720656523304264\n",
      "Epoch: 982, train_Loss: 1.795979967931422, val_Loss: 1.720656481972576\n",
      "Epoch: 983, train_Loss: 1.7959799557197385, val_Loss: 1.7206565650710224\n",
      "Epoch: 984, train_Loss: 1.7959799622617116, val_Loss: 1.720656523304264\n",
      "Epoch: 985, train_Loss: 1.7959799570281332, val_Loss: 1.720656481972576\n",
      "Epoch: 986, train_Loss: 1.7959799581911506, val_Loss: 1.7206565163431375\n",
      "Epoch: 987, train_Loss: 1.7959799599356767, val_Loss: 1.7206565163431375\n",
      "Epoch: 988, train_Loss: 1.7959799590634136, val_Loss: 1.7206565206938416\n",
      "Epoch: 989, train_Loss: 1.7959799592087908, val_Loss: 1.720656481972576\n",
      "Epoch: 990, train_Loss: 1.7959799560104928, val_Loss: 1.720656481972576\n",
      "Epoch: 991, train_Loss: 1.7959799648785009, val_Loss: 1.720656481972576\n",
      "Epoch: 992, train_Loss: 1.7959799610986942, val_Loss: 1.720656481972576\n",
      "Epoch: 993, train_Loss: 1.7959799576096418, val_Loss: 1.720656481972576\n",
      "Epoch: 994, train_Loss: 1.795979958481905, val_Loss: 1.720656481972576\n",
      "Epoch: 995, train_Loss: 1.7959799469971076, val_Loss: 1.7206566033572177\n",
      "Epoch: 996, train_Loss: 1.7959799603718083, val_Loss: 1.720656481972576\n",
      "Epoch: 997, train_Loss: 1.7959799589180365, val_Loss: 1.7206566033572177\n",
      "Epoch: 998, train_Loss: 1.7959799580457734, val_Loss: 1.7206566033572177\n",
      "Epoch: 999, train_Loss: 1.795979967931422, val_Loss: 1.720656523304264\n",
      "Epoch: 1000, train_Loss: 1.7959799574642648, val_Loss: 1.7206566033572177\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_Predictor(dataset)\n",
    "model = model.to(device)\n",
    "train_losses, val_losses = train(dataset, train_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "27a7152e-3438-43f2-9432-f1b189df9404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predect(dataset, test_dataset, model):\n",
    "    model.eval()\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=5, drop_last=True)\n",
    "\n",
    "    index = np.random.choice(len(test_dataloader))\n",
    "    \n",
    "    for batch, (x, y) in enumerate(test_dataloader):\n",
    "    \n",
    "        smiles, wavelength = x, y\n",
    "        break\n",
    "    state_h, state_c = model.init_state(5)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    y_pred, (state_h, state_c) = model(smiles.to(device), (state_h, state_c))\n",
    "    y_pred_permute = torch.permute(y_pred, (2, 1, 0))    \n",
    "\n",
    "    print(y_pred_permute)\n",
    "    print(wavelength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "df41f473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0726, -0.1839, -0.1839, -0.1839, -0.1839],\n",
      "         [-0.1162, -0.5293, -0.7402, -0.5293, -0.5293],\n",
      "         [-0.2724, -1.0068, -1.3992, -1.0068, -1.1366],\n",
      "         [-0.5556, -1.4714, -1.7319, -1.4714, -1.6050],\n",
      "         [-1.0207, -1.7217, -1.8202, -1.7217, -1.7867],\n",
      "         [-1.4371, -1.8110, -1.8385, -1.8110, -1.8313],\n",
      "         [-1.6880, -1.8355, -1.8421, -1.8355, -1.8406],\n",
      "         [-1.7950, -1.8413, -1.8428, -1.8413, -1.8425],\n",
      "         [-1.8289, -1.8426, -1.8430, -1.8426, -1.8429],\n",
      "         [-1.8391, -1.8429, -1.8430, -1.8429, -1.8430],\n",
      "         [-1.8420, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8427, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8429, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430],\n",
      "         [-1.8430, -1.8430, -1.8430, -1.8430, -1.8430]]], device='cuda:0',\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "tensor([-1.2182, -2.4815, -3.0969, -3.5227, -1.5654])\n"
     ]
    }
   ],
   "source": [
    "predect(dataset, test_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cc12867b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\n",
    "axes[0].plot(train_losses, label=\"train_Loss\")\n",
    "axes[0].plot(val_losses, label=\"val_Loss\")\n",
    "axes[0].grid()\n",
    "axes[0].set_xlabel(\"epoch\")\n",
    "axes[0].legend()\n",
    "\n",
    "\n",
    "axes[1].plot(train_losses, label=\"train_Loss\")\n",
    "axes[1].plot(val_losses, label=\"val_Loss\")\n",
    "axes[1].grid()\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].set_xlabel(\"epoch\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "91fdc21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataset, test_dataset, model):\n",
    "    model.eval()\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    batch_size = 1\n",
    "    \n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, drop_last=True)\n",
    "    \n",
    "    criterion = torch.nn.L1Loss()\n",
    "    \n",
    "    total_loss = 0\n",
    "    state_h, state_c = model.init_state(batch_size)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    \n",
    "    for batch, (x, y) in enumerate(test_dataloader):\n",
    "    \n",
    "        y_pred, (state_h, state_c) = model(x.to(device), (state_h, state_c))\n",
    "        y_pred_permute = torch.permute(y_pred, (2, 1, 0))    \n",
    "        loss = criterion(y_pred_permute[0, dataset.max_length-1], y.to(device))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        state_h = state_h.detach()\n",
    "        state_c = state_c.detach()\n",
    "    \n",
    "    average_total_loss = total_loss / len(test_dataloader)\n",
    "    \n",
    "    return average_total_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ce14c760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_Loss: 1.019\n"
     ]
    }
   ],
   "source": [
    "print(\"test_Loss: {:.3f}\".format(test(dataset, test_dataset, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "64cfc140",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "y_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "state_h, state_c = model.init_state(batch_size)\n",
    "state_h = state_h.to(device)\n",
    "state_c = state_c.to(device)\n",
    "\n",
    "for batch, (x, y) in enumerate(test_dataloader):\n",
    "\n",
    "    y_pred, (state_h, state_c) = model(x.to(device), (state_h, state_c))\n",
    "    y_pred_permute = torch.permute(y_pred, (2, 1, 0))    \n",
    "    state_h = state_h.detach()\n",
    "    state_c = state_c.detach()\n",
    "    \n",
    "    y_list.append(y.cpu().detach().numpy())\n",
    "    y_pred_list.append(y_pred_permute[0, dataset.max_length-1].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4e3172f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネルがクラッシュしました。エラーの原因を特定するには、セル内のコードを確認してください。詳細については、<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a> をクリックしてください。さらなる詳細については、Jupyter [log] (command:jupyter.viewOutput) を参照してください。"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(y_list, y_pred_list, s=1)\n",
    "plt.title(REGRESSION_COL)\n",
    "plt.xlabel(\"labels\")\n",
    "plt.ylabel(\"output\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f18a3795fe5cb6a973482da1273789e92c21cec72867792a35f0dcfcd5bea518"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
